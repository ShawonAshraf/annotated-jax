{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning a pretrained model from Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained models are great. They're trained on a lot of data us normies probably won't be able to compile by ourselves and they also require a lot of compute to train from scratch. Ever since BERT was released, the NLP community has been using pre-trained models to fine-tune on their own datasets. This is a great way to leverage the power of these models without having to train them from scratch.\n",
    "\n",
    "(The last two sentences were suggested by Copilot. I don't disagree but don't blame me for plagiarism.)\n",
    "\n",
    "So to pay homage to the model that brought the ImageNet moment to NLP, I will show you how you can take a pre-trained BERT model from Huggingface and train it on a dataset for movie reviews.\n",
    "\n",
    "The dataset can be found here: [Pang & Lee, 2004](http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz)\n",
    "\n",
    "Also, if you're interested in the paper behind the dataset:\n",
    "\n",
    "```bibtex\n",
    "@inproceedings{pang-lee-2004-sentimental,\n",
    "    title = \"A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts\",\n",
    "    author = \"Pang, Bo  and\n",
    "      Lee, Lillian\",\n",
    "    booktitle = \"Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04)\",\n",
    "    month = jul,\n",
    "    year = \"2004\",\n",
    "    address = \"Barcelona, Spain\",\n",
    "    url = \"https://aclanthology.org/P04-1035\",\n",
    "    doi = \"10.3115/1218955.1218990\",\n",
    "    pages = \"271--278\",\n",
    "}\n",
    "```\n",
    "\n",
    "To keep the main focus on the fine-tuning process, I will abstract the data preprocessing in a separate python script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.pre_polarity import prepare_dataset\n",
    "from IPython.display import clear_output\n",
    "\n",
    "main_dataset = prepare_dataset()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-07 02:58:23.659\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mTotal dataset size: 2000\u001b[0m\n",
      "\u001b[32m2024-07-07 02:58:23.660\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mCreating Train and Test Splits.\u001b[0m\n",
      "\u001b[32m2024-07-07 02:58:23.664\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mTrain dataset size: 1600\u001b[0m\n",
      "\u001b[32m2024-07-07 02:58:23.664\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mTest dataset size: 400\u001b[0m\n",
      "\u001b[32m2024-07-07 02:58:23.665\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mCreating Train Dev Split from Train Dataset.\u001b[0m\n",
      "\u001b[32m2024-07-07 02:58:23.667\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m20\u001b[0m - \u001b[1mTrain dataset size: 1280\u001b[0m\n",
      "\u001b[32m2024-07-07 02:58:23.668\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mDev dataset size: 320\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from loguru import logger\n",
    "\n",
    "logger.info(f\"Total dataset size: {len(main_dataset)}\")\n",
    "logger.info(\"Creating Train and Test Splits.\")\n",
    "train_test_dict = main_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "train_dataset = train_test_dict[\"train\"]\n",
    "test_dataset = train_test_dict[\"test\"]\n",
    "\n",
    "logger.info(f\"Train dataset size: {len(train_dataset)}\")\n",
    "logger.info(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "logger.info(\"Creating Train Dev Split from Train Dataset.\")\n",
    "train_dev_dict = train_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "\n",
    "train_dataset = train_dev_dict[\"train\"]\n",
    "dev_dataset = train_dev_dict[\"test\"]\n",
    "\n",
    "logger.info(f\"Train dataset size: {len(train_dataset)}\")\n",
    "logger.info(f\"Dev dataset size: {len(dev_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import Dataset as HFDataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "class PolarityReviewDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_split: HFDataset, \n",
    "                 tokenizer_model_name: str = \"google-bert/bert-base-uncased\",\n",
    "                 max_len: int = 512):\n",
    "        self.ds = dataset_split\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name)\n",
    "        self.MAX_LEN = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.ds[idx][\"text\"]\n",
    "        label = self.ds[idx][\"label\"]\n",
    "\n",
    "        # encode review text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.MAX_LEN,\n",
    "            truncation=True,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"np\" # return numpy arrays\n",
    "        )\n",
    "        \n",
    "        return encoding[\"input_ids\"], encoding[\"attention_mask\"], np.array([label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = PolarityReviewDataset(train_dataset)\n",
    "devset = PolarityReviewDataset(dev_dataset)\n",
    "testset = PolarityReviewDataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax_dataloader as jdl\n",
    "\n",
    "BATCH_SIZE = 24 # Max I could load on an RTX 3090\n",
    "train_loader = jdl.DataLoader(\n",
    "    trainset, \"pytorch\", batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = jdl.DataLoader(\n",
    "    devset, \"pytorch\", batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = jdl.DataLoader(\n",
    "    testset, \"pytorch\", batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition\n",
    "\n",
    "I would urge you to pay special attention to this part if you're coming from pytorch. Jax works differently. So does Flax. Although BERT is available as a Flax module on the HF hub, the loading process is different than that of the pytorch version. \n",
    "\n",
    "First of all, Flax models are immutable pytrees. Pytorch models are a container of tensors which can be mutated. So you can update or assign new params to a Pytorch model on the fly. The same is not possible with Flax models. \n",
    "\n",
    "Second, you can't take a Flax model with pretrained params and just assign it to a flax model with the same architecture. You have to unfreeze the new model params, then overwrite them with the pretrained params and then freeze them again. It's like opening a pack of chips and sealing it back again so that nobody knows that you ate some.\n",
    "\n",
    "Let's check some code first then I will explain.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import FlaxAutoModel\n",
    "\n",
    "\n",
    "def load_model(model_name: str = \"google-bert/bert-base-uncased\") -> tuple:\n",
    "    model = FlaxAutoModel.from_pretrained(model_name)\n",
    "    clear_output()\n",
    "    \n",
    "    # extract the module and the params\n",
    "    module = model.module\n",
    "    pretrained_params = {\"params\": model.params}\n",
    "    \n",
    "    return module, pretrained_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, I extracted the flax module and the params from the model. Now I will define a new model and assign these params to that one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "from flax.core.frozen_dict import unfreeze, freeze\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "\n",
    "class SentimentCLF(nn.Module):\n",
    "    backbone: nn.Module # the pretrained model\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, input_ids: jnp.ndarray, attention_mask: jnp.ndarray) -> jnp.ndarray:\n",
    "        # forward pass\n",
    "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # pooler_output\n",
    "        out = out.pooler_output\n",
    "        \n",
    "        # pass through a dense layer that projects to 2 labels types\n",
    "        out = nn.Dense(2)(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_module, pretrained_params = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "rng = jax.random.key(42)\n",
    "model = SentimentCLF(bert_module)\n",
    "\n",
    "sample_data = trainset[0]\n",
    "input_ids, attention_mask, label = sample_data\n",
    "\n",
    "params = model.init(rng, input_ids, attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfreeze and Freeze.\n",
    "\n",
    "![Mr_freeze.png](./images/freeze.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfreeze the new model\n",
    "params_unfrozen = unfreeze(params)\n",
    "params_unfrozen[\"params\"][\"backbone\"] = pretrained_params[\"params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze back\n",
    "params = freeze(params_unfrozen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. Now you can train this model as any other flax model.\n",
    "\n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "@jax.jit\n",
    "def calculate_loss(params, input_ids, attention_mask, label):\n",
    "    logits = model.apply(params, input_ids, attention_mask)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, label)\n",
    "    # typical numpy array thing\n",
    "    # should be a scalar\n",
    "    return loss[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def batched_loss(params, input_ids, attention_masks, labels):\n",
    "    batch_loss = jax.vmap(calculate_loss, in_axes=(None, 0, 0, 0))(\n",
    "        params, input_ids, attention_masks, labels)\n",
    "    return batch_loss.mean(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training import train_state\n",
    "\n",
    "clipper = optax.clip_by_global_norm(1.0)\n",
    "\n",
    "tx = optax.chain(optax.adam(learning_rate=2e-5),\n",
    "                 optax.clip_by_global_norm(1.0))\n",
    "\n",
    "initial_state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    tx=tx,\n",
    "    params=params,\n",
    ")\n",
    "criterion = jax.value_and_grad(batched_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def test_step(state, batch):\n",
    "    input_ids, attention_masks, _ = batch\n",
    "\n",
    "    def infer(params, input_ids, attention_mask):\n",
    "        logits = model.apply(params, input_ids, attention_mask)\n",
    "        return jax.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    probas = jax.vmap(jax.jit(infer), in_axes=(None, 0, 0))(\n",
    "        state.params, input_ids, attention_masks)\n",
    "\n",
    "    return probas\n",
    "\n",
    "\n",
    "def evaluate(state, test_loader):\n",
    "    scores = list()\n",
    "    for batch in tqdm(test_loader):\n",
    "        labels = batch[2]\n",
    "        probas = test_step(state, batch)\n",
    "        preds = np.argmax(probas, axis=-1)\n",
    "        # f1 score, never trust simple accuracy!\n",
    "        f1s = f1_score(labels, preds)\n",
    "\n",
    "        scores.append(f1s)\n",
    "\n",
    "    return np.array(scores).mean(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    input_ids, attention_masks, labels = batch\n",
    "    loss_value, grads = criterion(\n",
    "        state.params, input_ids, attention_masks, labels)\n",
    "    updated_state = state.apply_gradients(grads=grads)\n",
    "    return loss_value, updated_state\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def validation_step(state, batch):\n",
    "    input_ids, attention_masks, labels = batch\n",
    "    loss_value, _ = criterion(state.params, input_ids, attention_masks, labels)\n",
    "    return loss_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "507f3d8d4cd9487cb2c1b0c7c35126e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501f3ccaa672423083d71b669187e527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_step:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c81a58e74a14fceab0f96182f0d0367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation_step:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-07 02:59:13.291\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mEpoch : 1 :: Step : 40 :: Loss/Train : 0.5054243206977844 :: Loss/Validation : 0.3673432469367981\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdcb4f79124343e5b5dcd255e5b397c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_step:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38a5ea0a3e1465aa357db3709423189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation_step:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-07 02:59:40.769\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mEpoch : 2 :: Step : 80 :: Loss/Train : 0.23009130358695984 :: Loss/Validation : 0.3605358302593231\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def train(state, epochs, train_loader, val_loader):\n",
    "    steps = 0\n",
    "    train_losses = []\n",
    "    mean_val_losses = []\n",
    "\n",
    "    # =============\n",
    "    for e in trange(epochs):\n",
    "        for batch in tqdm(train_loader, desc=\"train_step\"):\n",
    "            train_loss, state = train_step(state, batch)\n",
    "            steps += 1\n",
    "\n",
    "            # log every 200 steps\n",
    "            if steps % 40 == 0:\n",
    "                train_losses.append(train_loss)\n",
    "\n",
    "                # run validation\n",
    "                validation_losses = []\n",
    "                for batch in tqdm(val_loader, desc=\"validation_step\"):\n",
    "                    val_loss = validation_step(state, batch)\n",
    "                    validation_losses.append(val_loss)\n",
    "\n",
    "                mean_val_loss = np.array(validation_losses).mean(axis=-1)\n",
    "                mean_val_losses.append(mean_val_loss)\n",
    "\n",
    "                logger.info(\n",
    "                    f\"Epoch : {e + 1} :: Step : {steps} :: Loss/Train : {train_loss} :: Loss/Validation : {mean_val_loss}\")\n",
    "\n",
    "    # ============\n",
    "    return state, train_losses, mean_val_losses\n",
    "\n",
    "\n",
    "# ============\n",
    "trained_state, train_losses, mean_val_losses = train(\n",
    "    initial_state, 2, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kinda sus, looks like slight ovefitting but let's do an eval first and then I will explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Always evaluate your models. You don't leave home without brushing teeth in the morning, do you?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0396d593bba741dfb387b692b0adf992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-07 02:59:58.041\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m2\u001b[0m - \u001b[1mTest F1 Score: 0.8734399378232043\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "score = evaluate(trained_state, test_loader)\n",
    "logger.info(f\"Test F1 Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main problem with this dataset is that the inputs are longer than what BERT can handle (512). In the tokeniser I added truncation. This leads to information loss, so the model is basically reading halfway through the texts and is forced to make a hasty decision about the label. \n",
    "\n",
    "Either way, our goal was to fine tune a model, we did that. In a real life scenario, always check your data and what you want your model to get out of it and how before you burn electricity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-examples-0E04DKSd-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
