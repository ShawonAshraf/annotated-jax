{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c49b7230",
   "metadata": {},
   "source": [
    "### The magic sauce over numpy\n",
    "\n",
    "Transforms are what Jax adds on top of numpy. This is more of an advanced thing to understand and if you wish to skip and come back later, you can do it. It's fine\n",
    "\n",
    "So about transforms in Jax. There are these transforms: `jit`, `grad`, `pmap`, `vmap`.\n",
    "\n",
    "This notebook is about jit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce8614d",
   "metadata": {},
   "source": [
    "### jit()\n",
    "\n",
    "Jax calls itself accelerated numpy. Or a faster numpy. Or however you interprete it. jit is one of the key components in this acceleration. What jit does is that it keeps a copy of your functions in the cache which can be executed faster than a regular function call. But how? \n",
    "\n",
    "\n",
    "\n",
    "#### Translating computer programs (or a rather CS101 refresher)\n",
    "\n",
    "Computers only understand 1's and 0's (in other words, binary numbers). So when you try to run your code, it needs to be translated into binary numbers first. This process isn't straightforward. There are multiple levels of what CS junkies call abstractions. One possible abstraction hierarchy can be like this (or what it used to look like before [llvm](https://llvm.org/) and [clang](https://clang.llvm.org/) came). \n",
    "\n",
    "\n",
    "```\n",
    "-------> Your code \n",
    "        -------> **Translator**\n",
    "                -------> Assembler (based on the instruction Set that your CPU / GPU maker made, also known as ISA) \n",
    "                        -------> Binary (executable)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "This translator part in the middle can be either a Compiler or an Interpreter. Compiler takes your entire code file and translates it at once. There's a whole field of study in CS on compiler design if you want to look into that abyss. Anyway, since compilers translate or compile the whole file at once, you can add various optimizations (memory, speed) and enforce checks at translation or compile time to make sure your code has reduced amount of errors. Compiled programs are also, by comparison, faster. (Video Games, Operating Systems are prime examples.) **AND you need to compile only once!** (terms and conditions apply)\n",
    "\n",
    "Python on the other hand is interpreted. Interpreter translates one line at a time and **you have to intereprete everytime** you want to run your code. The process is slow and you can't enforce the same checks and optimizations like a compiler. Everything is known when the code is encountered so errors can't be reduced beforehand. So when Jax tries to cache your code for faster recall, the regular python design holds it back. Instead, Jax uses a compiler called XLA, which compiles your python code and caches it.\n",
    "\n",
    "\n",
    "And oh, JIT means just in time compilation (thank me later :) )\n",
    "\n",
    "So let's check how much of a performance benefit jit adds on top of regular code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77c46ac5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T01:12:34.348010Z",
     "start_time": "2023-02-08T01:12:34.116136Z"
    }
   },
   "outputs": [],
   "source": [
    "import jax as J\n",
    "from jax import jit, random\n",
    "import jax.numpy as jnp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb65fb7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T01:12:35.714470Z",
     "start_time": "2023-02-08T01:12:35.305384Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-0.10502207, -0.56205004, -0.56485987, -1.7063935 , -1.3647023 ,\n",
       "       -0.42215332,  1.0077653 ,  0.9922631 , -0.61236995], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = random.key(123)\n",
    "\n",
    "x = random.normal(key, shape=(9, ))\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29a9bb95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T01:12:42.052635Z",
     "start_time": "2023-02-08T01:12:37.961241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.3 µs ± 79.2 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return jnp.sin(x) + jnp.cos(x)\n",
    "\n",
    "%timeit f(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef095714",
   "metadata": {},
   "source": [
    "With jit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5a7a596",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T01:12:57.421263Z",
     "start_time": "2023-02-08T01:12:45.502012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.3 µs ± 36.1 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "f_with_jit = jit(f)\n",
    "\n",
    "%timeit f_with_jit(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7497cc1",
   "metadata": {},
   "source": [
    "See that wall time difference ? That's like real fast!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe078dbb",
   "metadata": {},
   "source": [
    "#### Alternate syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cefd33f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T01:13:20.152293Z",
     "start_time": "2023-02-08T01:13:08.452267Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.4 µs ± 40.5 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def f(x):\n",
    "    return jnp.sin(x) + jnp.cos(x)\n",
    "\n",
    "%timeit f(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1c5b11",
   "metadata": {},
   "source": [
    "### But it's not all sunshine....\n",
    "\n",
    "Well caching and speeding up is nice but if your code changes, jit will have to compile it again. This adds some overhead and repeated compilations can make code execution slow, for example inside a loop. Also, not everything in python is supported in jax transformations, such as jit. Your code should be side-effect free (for starters, not have pythonic `loops`, `print` statements and `if-else` conditionals) and jax enforces a very functional programming approach. As a result you may not be able to jit every part of your code. \n",
    "\n",
    "This is where jax is different from Tensorflow or Pytorch. You've to think which parts of your code are going to be covered by transformations, which parts only take care of computations and which parts take care of loops and conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036864e3",
   "metadata": {},
   "source": [
    "#### What my python code becomes after XLA compilation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b12f4a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T01:13:30.340764Z",
     "start_time": "2023-02-08T01:13:30.339058Z"
    }
   },
   "outputs": [],
   "source": [
    "from jax import make_jaxpr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05e20425",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T01:13:34.179960Z",
     "start_time": "2023-02-08T01:13:34.175268Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[9]. let\n",
       "    b:f32[9] = pjit[\n",
       "      jaxpr={ lambda ; c:f32[9]. let\n",
       "          d:f32[9] = sin c\n",
       "          e:f32[9] = cos c\n",
       "          f:f32[9] = add d e\n",
       "        in (f,) }\n",
       "      name=f\n",
       "    ] a\n",
       "  in (b,) }"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_jaxpr(f)(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf05cc39",
   "metadata": {},
   "source": [
    "This is what XLA does to your code and what Jax will see while executing it from a cache when flagged with jit.\n",
    "\n",
    "#### Now what if you do include some side effects or code unwanted by jax, for example a print statement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "453260f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T01:21:22.940168Z",
     "start_time": "2023-02-08T01:21:22.935314Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traced<ShapedArray(float32[9])>with<DynamicJaxprTrace(level=2/0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[9] b:f32[9]. let\n",
       "    c:f32[9] = pjit[\n",
       "      jaxpr={ lambda ; d:f32[9] e:f32[9]. let f:f32[9] = add d e in (f,) }\n",
       "      name=side_effect_func\n",
       "    ] a b\n",
       "  in (c,) }"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jit\n",
    "def side_effect_func(x, y):\n",
    "    print(x)\n",
    "    x = x + y # yes this is also a side effect\n",
    "    return x\n",
    "\n",
    "make_jaxpr(side_effect_func)(x, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e8b7dc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-08T01:21:33.181956Z",
     "start_time": "2023-02-08T01:21:33.177471Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda ; a:f32[9] b:f32[9]. let\n",
       "    c:f32[9] = pjit[\n",
       "      jaxpr={ lambda ; d:f32[9] e:f32[9]. let f:f32[9] = add d e in (f,) }\n",
       "      name=without_side_effect\n",
       "    ] a b\n",
       "  in (c,) }"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jit\n",
    "def without_side_effect(x, y):\n",
    "    return x + y\n",
    "\n",
    "make_jaxpr(without_side_effect)(x, x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c69ca1d",
   "metadata": {},
   "source": [
    "See that there's `Traced<ShapedArray(float32[9])>with<DynamicJaxprTrace(level=1/1)>` in the jaxpr output for the function with side effects. So instead of being a pure jax type, it now has a tracer type, which has to adapt to the induced side effects, the print statement and modification of x. \n",
    "\n",
    "This may not be alarm bells to you as long as you're getting correct results but this may result in inconsistent behavior acorss devices. (which jax wants to avoid at all costs). \n",
    "\n",
    "You can read more here about [jax insisting on pure functions](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#pure-functions).\n",
    "\n",
    "TL;DR : Just know that Jax is functional at core and only likes pure functions in your code. Anything else will probably run but jax can't provide you any guarantee about correct or consistent results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if I want some parameters of my function to be not compiled?\n",
    "\n",
    "You may encounter this scenario if your function has to build an array with a shape which is not known prior (supplied as a parameter). Consider the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([6, 7, 7, 6, 0, 1, 6, 2, 6, 7, 6, 6, 6, 0, 7, 1, 1, 0, 2, 6, 0, 7,\n",
       "       8, 6, 6, 6, 7, 3, 6, 6, 1, 3, 7, 7, 4, 8, 6, 7, 8, 1, 7, 6, 6, 7,\n",
       "       7, 2, 2, 6, 6, 6], dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multinomial_sample(rng_key, logits, n_samples):\n",
    "    return random.categorical(rng_key, logits, shape=[n_samples, ])\n",
    "\n",
    "key, subkey = random.split(key)\n",
    "multinomial_sample(subkey, x, 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jit'ing this function is not allowed, since `n_samples` can change and the array returned from the function won't have a fixed size on each call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Shapes must be 1D sequences of concrete values of integer type, got (Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>,).\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe error occurred while tracing the function multinomial_sample at /tmp/ipykernel_5821/897393480.py:1 for jit. This concrete value was not available in Python because it depends on the value of the argument n_samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/shawon/Projects/jax_examples/05_jit.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bnot-real-slim-shady/home/shawon/Projects/jax_examples/05_jit.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m jit(multinomial_sample)(subkey, x, \u001b[39m50\u001b[39;49m)\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "\u001b[1;32m/home/shawon/Projects/jax_examples/05_jit.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bnot-real-slim-shady/home/shawon/Projects/jax_examples/05_jit.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmultinomial_sample\u001b[39m(rng_key, logits, n_samples):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bnot-real-slim-shady/home/shawon/Projects/jax_examples/05_jit.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m random\u001b[39m.\u001b[39;49mcategorical(rng_key, logits, shape\u001b[39m=\u001b[39;49m[n_samples, ])\n",
      "File \u001b[0;32m~/miniconda3/envs/jax_examples/lib/python3.10/site-packages/jax/_src/random.py:1342\u001b[0m, in \u001b[0;36mcategorical\u001b[0;34m(key, logits, axis, shape)\u001b[0m\n\u001b[1;32m   1340\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1341\u001b[0m   shape \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(shape)\n\u001b[0;32m-> 1342\u001b[0m   _check_shape(\u001b[39m\"\u001b[39;49m\u001b[39mcategorical\u001b[39;49m\u001b[39m\"\u001b[39;49m, shape, batch_shape)\n\u001b[1;32m   1344\u001b[0m shape_prefix \u001b[39m=\u001b[39m shape[:\u001b[39mlen\u001b[39m(shape)\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(batch_shape)]\n\u001b[1;32m   1345\u001b[0m logits_shape \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(shape[\u001b[39mlen\u001b[39m(shape) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(batch_shape):])\n",
      "File \u001b[0;32m~/miniconda3/envs/jax_examples/lib/python3.10/site-packages/jax/_src/random.py:232\u001b[0m, in \u001b[0;36m_check_shape\u001b[0;34m(name, shape, *param_shapes)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_shape\u001b[39m(name: \u001b[39mstr\u001b[39m, shape: Union[Shape, NamedShape], \u001b[39m*\u001b[39mparam_shapes) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 232\u001b[0m   shape \u001b[39m=\u001b[39m core\u001b[39m.\u001b[39;49mas_named_shape(shape)\n\u001b[1;32m    234\u001b[0m   \u001b[39mif\u001b[39;00m param_shapes:\n\u001b[1;32m    235\u001b[0m     shape_ \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39mbroadcast_shapes(shape\u001b[39m.\u001b[39mpositional, \u001b[39m*\u001b[39mparam_shapes)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/jax_examples/lib/python3.10/site-packages/jax/_src/core.py:2024\u001b[0m, in \u001b[0;36mcanonicalize_shape\u001b[0;34m(shape, context)\u001b[0m\n\u001b[1;32m   2022\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   2023\u001b[0m   \u001b[39mpass\u001b[39;00m\n\u001b[0;32m-> 2024\u001b[0m \u001b[39mraise\u001b[39;00m _invalid_shape_error(shape, context)\n",
      "\u001b[0;31mTypeError\u001b[0m: Shapes must be 1D sequences of concrete values of integer type, got (Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=1/0)>,).\nIf using `jit`, try using `static_argnums` or applying `jit` to smaller subfunctions.\nThe error occurred while tracing the function multinomial_sample at /tmp/ipykernel_5821/897393480.py:1 for jit. This concrete value was not available in Python because it depends on the value of the argument n_samples."
     ]
    }
   ],
   "source": [
    "jit(multinomial_sample)(subkey, x, 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution here is to tell jit that you want it to ignore the n_samples parameter. For that you have to use `static_argnums`. \n",
    "\n",
    "You can read more at: https://jax.readthedocs.io/en/latest/_autosummary/jax.jit.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([6, 7, 7, 6, 0, 1, 6, 2, 6, 7, 6, 6, 6, 0, 7, 1, 1, 0, 2, 6, 0, 7,\n",
       "       8, 6, 6, 6, 7, 3, 6, 6, 1, 3, 7, 7, 4, 8, 6, 7, 8, 1, 7, 6, 6, 7,\n",
       "       7, 2, 2, 6, 6, 6], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jit(multinomial_sample, static_argnums=2)(subkey, x, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([7, 7, 7, 7, 7, 0, 7, 7, 6, 6, 8, 6, 4, 2, 7, 6, 6, 7, 7, 7, 7, 3,\n",
       "       6, 1, 7, 2, 7, 3, 6, 7, 6, 7, 6, 8, 7, 7, 0, 6, 0, 5, 7, 7, 7, 0,\n",
       "       6, 7, 6, 0, 2, 5, 6, 6, 5, 7, 8, 7, 0, 7, 4, 2, 0, 6, 3, 6, 0, 7,\n",
       "       2, 0, 7, 0, 2, 5, 2, 0, 7, 6, 8, 2, 7, 7, 7, 2, 7, 7, 7, 6, 0, 6,\n",
       "       5, 6, 7, 0, 8, 5, 6, 6, 2, 6, 2, 0, 3, 7, 6, 8, 0, 6, 7, 8, 0, 7,\n",
       "       4, 6, 6, 6, 7, 6, 7, 5, 6, 6, 7, 5, 7, 6, 3, 7, 7, 6, 7, 6, 7, 7,\n",
       "       7, 0, 6, 4, 7, 7, 6, 6, 0, 7, 0, 6, 6, 7, 7, 6, 6, 7], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jit(multinomial_sample, static_argnums=2)(subkey, x, 150)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "85f6f424c393e7c95d987d0a64d03300802113ad2045bd4cdb9a90d6d84115f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
