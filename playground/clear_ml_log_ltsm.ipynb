{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff0775ef-abeb-45cb-9f07-3371ba712ff5",
   "metadata": {},
   "source": [
    "# A Parts of Speech Tagger with LSTM\n",
    "\n",
    "Now that we know how to use a dataloader, define flax modules and set up training pipelines, let's look at an actual example which will combine all of these and also take a peek at the LSTM class in flax.\n",
    "\n",
    "For this task, we're going to use the `batterydata/pos_tagging` dataset from Huggingface Datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd0e4b6-95eb-4d31-932b-e0dd8b4b5ac4",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "This particular dataset comes with train and test splits and no validation splits. Ideally, we should have all the three splits in a model training pipeline. So one solution here is to break the train split and create train and validation splits from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1844daa-87f7-4e77-97e0-07af425d668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "main_dataset = load_dataset(\"batterydata/pos_tagging\")\n",
    "train_split = main_dataset[\"train\"]\n",
    "test_split = main_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "788a776c-a04a-4b7f-8448-8b7d446d320e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_validation_splits(dataset_split,\n",
    "                                 validation_size = 0.2,\n",
    "                                 seed: int = 42):\n",
    "    # make a copy of the data\n",
    "    dataset_split = dataset_split.shuffle(seed=seed)\n",
    "    # using the train test split method to create validation set\n",
    "    dataset_split = dataset_split.train_test_split(test_size=validation_size,\n",
    "                                                   shuffle=True,\n",
    "                                                   seed=seed)\n",
    "    return dataset_split[\"train\"], dataset_split[\"test\"]\n",
    "\n",
    "\n",
    "train_split, validation_split = make_train_validation_splits(train_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a90a72d-2177-42e7-a680-d106828f751d",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "So we're going to take a very pre-Transformer era approach to preprocessing the dataset here. That means no byte pair tokenisation, no attention masks. We simple tokenise the words, use them as features. Which, also, practically leads us to the following steps:\n",
    "\n",
    "1. Tokenise\n",
    "2. Create a vocabulary\n",
    "3. Create a word to integer mapping (a dictionary will do)\n",
    "4. Create a label to integer mapping (pos tags are labels)\n",
    "5. Add out of vocabulary (OOV) and padding tokens\n",
    "\n",
    "\n",
    "One caveat here is that we have to use the main train split (without creating the validation split). But you probably have noticed by now that I've left a `main_dataset` variable up there. We're going to use that one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21cd5ee1-63aa-47e3-b80c-179f25d0f237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dict containing word -> idx mapping\n",
    "# pad token is 1 by default\n",
    "def map_word_to_idx(dataset_split, pad_token_idx = 1):\n",
    "    unique_words = set()\n",
    "    word_to_idx = dict()\n",
    "\n",
    "    # add an out of vocab token\n",
    "    oov_token = \"<OOV>\"\n",
    "    # pad token\n",
    "    pad_token = \"<PAD>\"\n",
    "\n",
    "    word_to_idx[oov_token] = 0\n",
    "    word_to_idx[pad_token] = pad_token_idx\n",
    "\n",
    "    # find the unique words\n",
    "    for data in dataset_split:\n",
    "        words = data[\"words\"]\n",
    "        for w in words:\n",
    "            unique_words.add(w)\n",
    "\n",
    "    # add index to them\n",
    "    for idx, uw in enumerate(list(unique_words)):\n",
    "        word_to_idx[uw] = idx + 2  # since oov is at 0 and pad at pad_token_idx\n",
    "\n",
    "    return word_to_idx\n",
    "\n",
    "word_to_idx = map_word_to_idx(main_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "244a5556-5a24-48b6-ba07-f189d7f3ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos tag -> idx mapping\n",
    "def map_label_to_idx(dataset, pad_token_idx = 1):\n",
    "    unique_labels = set()\n",
    "    label_to_idx = dict()\n",
    "\n",
    "    # add an out of vocab token\n",
    "    oov_token = \"<OOV>\"\n",
    "    # pad token\n",
    "    pad_token = \"<PAD>\"\n",
    "\n",
    "    label_to_idx[oov_token] = 0\n",
    "    label_to_idx[pad_token] = pad_token_idx\n",
    "\n",
    "    # find the unique labels\n",
    "    for data in dataset:\n",
    "        labels = data[\"labels\"]\n",
    "        for l in labels:\n",
    "            unique_labels.add(l)\n",
    "\n",
    "    # index\n",
    "    for idx, label in enumerate(list(unique_labels)):\n",
    "        label_to_idx[label] = idx + 2\n",
    "\n",
    "    return label_to_idx\n",
    "\n",
    "label_to_idx = map_label_to_idx(main_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342b1baa-d1b3-4d66-a560-f40e260a7822",
   "metadata": {},
   "source": [
    "Perfect! Now we have everything we need to create a Dataset class and Dataloaders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b813852d-592e-45aa-ad22-361f50319eac",
   "metadata": {},
   "source": [
    "## Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96baddf7-553b-428e-8c43-556fe6061815",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "\n",
    "class TagDataset(Dataset):\n",
    "    def __init__(self, dataset_split,\n",
    "                 pad_token_idx,\n",
    "                 max_seq_len,\n",
    "                 word_to_idx,\n",
    "                 label_to_idx) -> None:\n",
    "        self.dataset = dataset_split\n",
    "        self.pad_token_idx = pad_token_idx\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.label_to_idx = label_to_idx\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "\n",
    "    # use word_to_idx and label_to_idx to convert\n",
    "    # the string sequences to int sequences\n",
    "    def __encode(self, data_instance: dict) -> tuple:\n",
    "        words = data_instance[\"words\"]\n",
    "        labels = data_instance[\"labels\"]\n",
    "\n",
    "        # convert to int sequences\n",
    "        words = [self.word_to_idx.get(w, 0) for w in words]\n",
    "        labels = [self.label_to_idx.get(l) for l in labels]\n",
    "\n",
    "        return words, labels\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "        data = self.dataset[index]\n",
    "        words, labels = self.__encode(data)\n",
    "\n",
    "        # padding\n",
    "        words_padded = np.ones((self.max_seq_len,), dtype=np.int32) * self.pad_token_idx\n",
    "        words_padded[:len(words)] = words\n",
    "\n",
    "        labels_padded = np.ones((self.max_seq_len,), dtype=np.int32) * self.pad_token_idx\n",
    "        labels_padded[:len(labels)] = labels\n",
    "\n",
    "        # return padded words and labels\n",
    "        return words_padded, labels_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9125168-ce26-471e-9ab9-6f8e8de3deb6",
   "metadata": {},
   "source": [
    "Now we need to decide on the maximum sequence length. From some searching on the dataset, I found 300 to be a good value. You can also choose something around 280. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b7337cb-82a3-4af9-a553-44f9179710c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 300\n",
    "\n",
    "train_set = TagDataset(train_split, 1, max_seq_len, word_to_idx, label_to_idx)\n",
    "validation_set = TagDataset(validation_split, 1, max_seq_len, word_to_idx, label_to_idx)\n",
    "test_set = TagDataset(test_split, 1, max_seq_len, word_to_idx, label_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aaf586b3-2608-4d08-8d9a-574650382f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "import torch\n",
    "import jax_dataloader as jdl\n",
    "\n",
    "torch.manual_seed(2023)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = jdl.DataLoader(train_set, \"pytorch\", batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = jdl.DataLoader(validation_set, \"pytorch\", batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = jdl.DataLoader(test_set, \"pytorch\", batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677dbef2-49e8-4356-9e06-10b2f8a7bb6e",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "Time to define the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aba78987-04ae-4688-866f-5b8d3796f468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\"\"\"\n",
    "class LSTMTagger\n",
    "\n",
    "vocab_size: int, size of the vocabulary\n",
    "embedding_dimensions: int, size of the embedding dimensions\n",
    "lstm_hidden_dims: int, size of the lstm hidden dimensions\n",
    "n_labels: int, number of labels in the dataset (the padded size, e.g. if padding length is 100, then n_labels = 100)\n",
    "training: bool, whether the model is in training mode or not\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "    vocab_size: int\n",
    "    embedding_dimensions: int\n",
    "    lstm_hidden_dims: int\n",
    "    n_labels: int\n",
    "    lstm_seed: int\n",
    "    dropout_rate: float\n",
    "\n",
    "    def setup(self) -> None:\n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embed(\n",
    "            num_embeddings=self.vocab_size,\n",
    "            features=self.embedding_dimensions,\n",
    "            name=\"embedding\")\n",
    "\n",
    "        # lstm layer\n",
    "        self.lstm = nn.OptimizedLSTMCell(features=self.lstm_hidden_dims, name=\"lstm\")\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(self.dropout_rate, deterministic=True)\n",
    "\n",
    "        # dense layer\n",
    "        self.dense = nn.Dense(features=self.n_labels, name=\"dense\")\n",
    "\n",
    "    # lstm in flax: https://flax.readthedocs.io/en/latest/api_reference/flax.linen/_autosummary/flax.linen.LSTMCell.html\n",
    "    def __call__(self, words: jnp.ndarray) -> jnp.ndarray:\n",
    "        x = self.embedding(words)\n",
    "\n",
    "        carry = self.lstm.initialize_carry(random.key(self.lstm_seed), x.shape)\n",
    "        carry, x = self.lstm(carry=carry, inputs=x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.dense(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f60a97-ebdc-4071-834f-31bba3d6e6e1",
   "metadata": {},
   "source": [
    "Let's look at the model definition a bit. We have an embedding layer to create embedding vectors, followed by an LSTM which models the text sequences, a dropout layer and finally, a dense layer to map the lstm output to labels. \n",
    "\n",
    "One important thing to check here is how lstm works in flax. There are multiple implementations of a LSTMCell in flax. The one used here is a more optimised version. You can read more in the docs about it's implementation. In the forward pass of the model, you've to carry the initialize the carry or the state of the lstm for each batch or call and like any flax module, it requires a PRNG key. In other frameworks (e.g. Pytorch) you don't have to explcitly set the carry. \n",
    "\n",
    "Another is the dropout layer in flax. It requires its own PRNG, which you have to supply during the model init. Let's see how it's done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7abc181-341a-4c7d-b72c-f3f33639dff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the model\n",
    "SEED = 2023\n",
    "\n",
    "master_key = random.key(seed=SEED)\n",
    "master_key, model_init_key = random.split(master_key)\n",
    "master_key, dropout_key = random.split(master_key)\n",
    "model_rngs = {\"params\": model_init_key, \"dropout\": dropout_key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c0588fa-5c6b-4c61-900e-157068b365d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "        \"vocab_size\": len(word_to_idx),\n",
    "        \"embedding_dimensions\": 300,\n",
    "        \"lstm_hidden_dims\": 300,\n",
    "        \"n_labels\": len(label_to_idx),\n",
    "        \"lstm_seed\": 2024,\n",
    "        \"dropout_rate\": 0.2,\n",
    "}\n",
    "\n",
    "model = LSTMTagger(**model_config)\n",
    "init_params = model.init(model_rngs, np.arange(max_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a9110a3-c60e-44d9-9b82-bdc9b9988908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 50)\n"
     ]
    }
   ],
   "source": [
    "# a sample forward pass\n",
    "logits = model.apply(init_params, jnp.arange(300), rngs={\"dropout\": dropout_key})\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0931465-e7a0-4d2e-a7ce-098b1d69af1f",
   "metadata": {},
   "source": [
    "### Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74512e99-60a5-4fa1-9384-a048a720330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "from jax import vmap, jit\n",
    "\n",
    "# using cross entropy\n",
    "@jit\n",
    "def calculate_loss(params, words, labels):\n",
    "    logits = model.apply(params, words, rngs={\"dropout\": dropout_key})\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels)\n",
    "    return loss.mean(axis=-1)\n",
    "\n",
    "\n",
    "@jit\n",
    "def batched_loss(params, words_batched, labels_batched):\n",
    "    batch_loss = vmap(calculate_loss, in_axes=(None, 0, 0))(params, words_batched, labels_batched)\n",
    "    return batch_loss.mean(axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b905f14-2e10-4398-804b-dc09b4a977cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import trange, tqdm\n",
    "from flax.training import train_state\n",
    "from functools import partial\n",
    "from jax import value_and_grad\n",
    "\n",
    "optimiser = optax.adam(learning_rate=0.001)\n",
    "init_state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply, # the forward function\n",
    "    params=init_params,\n",
    "    tx=optimiser\n",
    ")\n",
    "criterion = value_and_grad(batched_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3db94ddc-cca4-461c-a6af-af32686db33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jit, static_argnums=0)\n",
    "def train_step(criterion, state, words_batched, labels_batched):\n",
    "    loss_value, grads = criterion(state.params, words_batched, labels_batched)\n",
    "    updated_state = state.apply_gradients(grads=grads)\n",
    "    return loss_value, updated_state\n",
    "\n",
    "\n",
    "@partial(jit, static_argnums=0)\n",
    "def validation_step(criterion, state, words_batched, labels_batched):\n",
    "    loss_value, _ = criterion(state.params, words_batched, labels_batched)\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d578d5-727b-4178-9f39-d16df81e13a3",
   "metadata": {},
   "source": [
    "## Orbax Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a850f3c-c998-4848-88a6-08545afb88fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import orbax\n",
    "from flax.training import orbax_utils\n",
    "\n",
    "# which will store the checkpoints\n",
    "orbax_checkpointer = orbax.checkpoint.PyTreeCheckpointer()\n",
    "\n",
    "# also have to define a save_args object from the initial state\n",
    "\n",
    "# checkpoint manager for managing how many checkpoints to keep\n",
    "options = orbax.checkpoint.CheckpointManagerOptions(max_to_keep=2, create=True)\n",
    "\n",
    "# save in /tmp now. will get cleared on reboot\n",
    "save_path = \"/tmp/flax_ckpt/orbax/managed\"\n",
    "\n",
    "checkpoint_manager = orbax.checkpoint.CheckpointManager(save_path, orbax_checkpointer, options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af94a593-93e7-49ed-9036-ea259ada7f49",
   "metadata": {},
   "source": [
    "### Logging: ClearML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8efaeba9-b873-4041-a442-15605dde72d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clearml\n",
    "clearml.browser_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75d5d013-d1cb-4a49-bb17-1094ac082d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=1c9de5daa40a407797563abc11bc8881\n",
      "2024-02-02 07:35:32,474 - clearml.Task - INFO - Storing jupyter notebook directly as code\n",
      "ClearML results page: https://app.clear.ml/projects/8e5be1efcfdc487e86bb77cd5a52222f/experiments/1c9de5daa40a407797563abc11bc8881/output/log\n",
      "ClearML Monitor: Could not detect iteration reporting, falling back to iterations as seconds-from-start\n"
     ]
    }
   ],
   "source": [
    "from clearml import Task\n",
    "\n",
    "task = Task.init(\n",
    "    project_name=\"jax-examples\",\n",
    "    task_name=\"pos-tagger\",\n",
    "    output_uri=True  # IMPORTANT: setting this to True will upload the model\n",
    "    # If not set the local path of the model will be saved instead!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca6d0c2f-42b0-4850-9999-757c83a49227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 24849,\n",
       " 'embedding_dimensions': 300,\n",
       " 'lstm_hidden_dims': 300,\n",
       " 'n_labels': 50,\n",
       " 'lstm_seed': 2024,\n",
       " 'dropout_rate': 0.2,\n",
       " 'learning_rate': 0.001,\n",
       " 'batch_size': 128}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    **model_config,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "}\n",
    "\n",
    "task.connect(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "923eedb8-c071-45a2-8f2d-b30b2967dd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = task.logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0c0ba45-187a-40dd-877d-8fce9aeafcf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SeriesInfo',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_close_stdout_handler',\n",
       " '_connect_logging',\n",
       " '_connect_std_streams',\n",
       " '_console',\n",
       " '_default_max_sample_history',\n",
       " '_default_upload_destination',\n",
       " '_flush_stdout_handler',\n",
       " '_flusher',\n",
       " '_get_tensorboard_auto_group_scalars',\n",
       " '_get_tensorboard_series_prefix',\n",
       " '_get_tensorboard_single_series_per_graph',\n",
       " '_get_used_title_series',\n",
       " '_graph_titles',\n",
       " '_parse_level',\n",
       " '_remove_std_logger',\n",
       " '_report_file_and_upload',\n",
       " '_report_image_plot_and_upload',\n",
       " '_report_worker',\n",
       " '_set_tensorboard_series_prefix',\n",
       " '_skip_console_log',\n",
       " '_start_task_if_needed',\n",
       " '_task',\n",
       " '_task_handler',\n",
       " '_tensorboard_logging_auto_group_scalars',\n",
       " '_tensorboard_series_force_prefix',\n",
       " '_tensorboard_single_series_per_graph',\n",
       " '_touch_title_series',\n",
       " 'capture_logging',\n",
       " 'current_logger',\n",
       " 'flush',\n",
       " 'get_default_debug_sample_history',\n",
       " 'get_default_upload_destination',\n",
       " 'get_flush_period',\n",
       " 'matplotlib_force_report_non_interactive',\n",
       " 'report_confusion_matrix',\n",
       " 'report_histogram',\n",
       " 'report_image',\n",
       " 'report_image_and_upload',\n",
       " 'report_line_plot',\n",
       " 'report_matplotlib_figure',\n",
       " 'report_matrix',\n",
       " 'report_media',\n",
       " 'report_plotly',\n",
       " 'report_scalar',\n",
       " 'report_scatter2d',\n",
       " 'report_scatter3d',\n",
       " 'report_single_value',\n",
       " 'report_surface',\n",
       " 'report_table',\n",
       " 'report_text',\n",
       " 'report_vector',\n",
       " 'set_default_debug_sample_history',\n",
       " 'set_default_upload_destination',\n",
       " 'set_flush_period',\n",
       " 'set_reporting_inf_value',\n",
       " 'set_reporting_nan_value',\n",
       " 'tensorboard_auto_group_scalars',\n",
       " 'tensorboard_single_series_per_graph']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0edbe0e7-ccd9-4e14-928b-63bb4ee3fa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create save args for checkpoint_manager\n",
    "\n",
    "ckpt = {\n",
    "    \"model\": init_state,\n",
    "    \"config\": model_config,\n",
    "    #\"model_prngs\": model_rngs\n",
    "}\n",
    "\n",
    "save_args = orbax_utils.save_args_from_target(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a3f1f2a8-4371-4958-ba85-a143f5347eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(state, train_loader, epochs=5, log_every_n_step=100):\n",
    "    step_counter = 0\n",
    "    train_losses = list()\n",
    "    mean_validation_losses = list()\n",
    "    train_acc = list()\n",
    "    val_acc = list()\n",
    "    test_acc = list()\n",
    "\n",
    "    for _ in trange(epochs):\n",
    "        for batch in train_loader:\n",
    "            loss, state = train_step(criterion, state, *batch)\n",
    "            step_counter += 1\n",
    "\n",
    "            if step_counter % log_every_n_step == 0:\n",
    "                # log train loss\n",
    "                train_losses.append(loss)\n",
    "                # run validation\n",
    "                vlosses = list()\n",
    "                for vbatch in val_loader:\n",
    "                    val_loss = validation_step(criterion, state, *vbatch)\n",
    "                    vlosses.append(val_loss)\n",
    "\n",
    "                vlosses = jnp.array(vlosses)\n",
    "                current_val_loss = vlosses.mean(axis=-1)\n",
    "\n",
    "                print(\n",
    "                    f\"Step [{step_counter + 1}] ---- Loss/Train :: {loss} ---- Loss/Val :: {current_val_loss}\")\n",
    "                logger.report_scalar(title=\"loss/train\", series=\"loss\", value=loss, iteration=step_counter)\n",
    "                logger.report_scalar(title=\"loss/val\", series=\"loss\", value=current_val_loss, iteration=step_counter)\n",
    "\n",
    "                # handle checkpointing\n",
    "                # save only if val loss decreased\n",
    "                if len(mean_validation_losses) != 0 and current_val_loss < mean_validation_losses[-1]:\n",
    "                    checkpoint_manager.save(step_counter, ckpt, save_kwargs={\"save_args\": save_args})\n",
    "                else:\n",
    "                    checkpoint_manager.save(step_counter, ckpt, save_kwargs={\"save_args\": save_args})\n",
    "\n",
    "                mean_validation_losses.append(current_val_loss)\n",
    "                \n",
    "\n",
    "\n",
    "    return state, train_losses, mean_validation_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdc13fc-529d-43d4-bea0-5a5a0afedeae",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2efb0607-f054-452f-b7c8-9ccda480554a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707c46a25c3f46faaba3b02ba43b9e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [101] ---- Loss/Train :: 0.2164287567138672 ---- Loss/Val :: 0.2119072824716568\n",
      "Step [201] ---- Loss/Train :: 0.06841154396533966 ---- Loss/Val :: 0.08331385254859924\n"
     ]
    }
   ],
   "source": [
    "state, train_loss, val_loss = train_model(init_state, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4701538e-a016-4c78-8bd4-1cd063989210",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We're going to measure the accuracy of the models. However, the outputs are padded, so we have to ignore the padded indexes, regardless of the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4670c616-a9ac-4753-96d0-41d745da1205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, actual, pad_idx=1):\n",
    "    non_padding_indices = jnp.nonzero((actual != pad_idx))\n",
    "\n",
    "    matches = jnp.equal(preds[non_padding_indices], actual[non_padding_indices])\n",
    "    acc = jnp.sum(matches) / actual[non_padding_indices].shape[0]\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4efd3148-d628-45e1-9f94-45150c8196d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.nn as jnn\n",
    "\n",
    "@jit\n",
    "def infer(params, words, labels):\n",
    "    logits = model.apply(params, words, rngs={\"dropout\": dropout_key})\n",
    "    proba = jnn.log_softmax(logits, axis=-1)\n",
    "    preds = jnp.argmax(proba, axis=-1)\n",
    "\n",
    "    return preds\n",
    "\n",
    "@jit\n",
    "def batch_infer(params, words, labels):\n",
    "    preds = vmap(infer, in_axes=(None, 0, 0))(params, words, labels)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "92c5c5c5-565d-44d5-8ae2-6914ae8fc5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(params, test_loader):\n",
    "    acc_per_batch = list()\n",
    "    for batch in tqdm(test_loader):\n",
    "        words, labels = batch\n",
    "        preds = batch_infer(params, words, labels)\n",
    "\n",
    "        acc = categorical_accuracy(preds, labels)\n",
    "        acc_per_batch.append(acc)\n",
    "\n",
    "    mean_acc = jnp.mean(jnp.array(acc_per_batch), axis=-1)\n",
    "\n",
    "    return mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "87d3323a-9bca-44ab-83f3-225e35d87c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e4177344914cdfa683edfac061dd9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test loader: 0.8367156982421875\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate(state.params, test_loader)\n",
    "print(f\"Accuracy on the test loader: {acc}\")\n",
    "logger.report_single_value(name=\"accuracy\", value=acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fd034aa2-7dc6-46c1-81e7-0a607914e1c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100.orbax-checkpoint-tmp-1706856375777864', '100', '200']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.listdir('/tmp/flax_ckpt/orbax/managed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ab01dd11-57be-4de6-a60d-e1be24f2cfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "latest_step = checkpoint_manager.latest_step()\n",
    "print(latest_step)\n",
    "\n",
    "saved_ckpt = checkpoint_manager.restore(latest_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d891f32e-0bb7-4731-b976-cf2c48ee35da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'dense': {'bias': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "         dtype=float32),\n",
       "   'kernel': array([[-0.09005692,  0.09999595, -0.04237846, ..., -0.1162146 ,\n",
       "           -0.1093234 ,  0.02832167],\n",
       "          [-0.06579961,  0.00771142,  0.09471543, ...,  0.10427108,\n",
       "            0.08416256,  0.12401416],\n",
       "          [-0.04541461,  0.09982762,  0.0263297 , ...,  0.02682773,\n",
       "            0.04141133,  0.03416595],\n",
       "          ...,\n",
       "          [ 0.00386063, -0.07003776,  0.02392506, ..., -0.07812898,\n",
       "           -0.07994714, -0.03207478],\n",
       "          [-0.05752759,  0.00412972, -0.02352364, ..., -0.02763494,\n",
       "            0.05712838,  0.05046622],\n",
       "          [ 0.00051849,  0.0105623 ,  0.07832165, ...,  0.09782899,\n",
       "           -0.12103408,  0.04564928]], dtype=float32)},\n",
       "  'embedding': {'embedding': array([[-0.02687831,  0.0247881 , -0.03601592, ...,  0.05882315,\n",
       "           -0.02189518, -0.0152519 ],\n",
       "          [-0.05561043, -0.0370508 , -0.05725506, ...,  0.04898517,\n",
       "           -0.00777896,  0.02112673],\n",
       "          [-0.05033677, -0.05459597,  0.02120498, ...,  0.02860671,\n",
       "           -0.00968476,  0.05396592],\n",
       "          ...,\n",
       "          [-0.06558979, -0.08430298,  0.02742645, ...,  0.06085121,\n",
       "            0.02937269, -0.06357964],\n",
       "          [ 0.07399348, -0.04649739,  0.00571184, ...,  0.01221033,\n",
       "           -0.01709292,  0.01288399],\n",
       "          [ 0.01983266,  0.05551459, -0.12402592, ...,  0.03773271,\n",
       "           -0.02007344, -0.03709742]], dtype=float32)},\n",
       "  'lstm': {'hf': {'bias': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       "    'kernel': array([[-0.06047809,  0.02120006,  0.03543405, ...,  0.00213962,\n",
       "            -0.06183857, -0.01153409],\n",
       "           [ 0.01285507,  0.06349862, -0.01597099, ...,  0.04713598,\n",
       "            -0.01410423,  0.11113632],\n",
       "           [-0.04851984, -0.06185386,  0.06012082, ...,  0.13515651,\n",
       "             0.0405813 ,  0.03513812],\n",
       "           ...,\n",
       "           [ 0.00531467,  0.02957071, -0.05986147, ..., -0.05700172,\n",
       "             0.03669643, -0.11331902],\n",
       "           [-0.03871503, -0.01024982,  0.09021022, ..., -0.05934833,\n",
       "            -0.06348281,  0.05578639],\n",
       "           [ 0.01762803, -0.00677029, -0.04248106, ..., -0.00431324,\n",
       "             0.07753044, -0.02333295]], dtype=float32)},\n",
       "   'hg': {'bias': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       "    'kernel': array([[ 0.06250548, -0.00525101,  0.10491331, ...,  0.00900862,\n",
       "             0.00679861, -0.01580323],\n",
       "           [-0.09777872, -0.08529794,  0.04785854, ...,  0.0857066 ,\n",
       "            -0.06257442, -0.0491118 ],\n",
       "           [ 0.00875059, -0.0093555 , -0.06478608, ..., -0.02576954,\n",
       "            -0.00281759,  0.02115513],\n",
       "           ...,\n",
       "           [ 0.04673006,  0.00245563, -0.0656492 , ..., -0.03872845,\n",
       "            -0.0828447 , -0.00907128],\n",
       "           [-0.06042923,  0.01860337,  0.11324456, ...,  0.01867498,\n",
       "            -0.03726216, -0.08979592],\n",
       "           [-0.08611488, -0.04312034, -0.00926692, ..., -0.04930711,\n",
       "             0.12374692, -0.00734815]], dtype=float32)},\n",
       "   'hi': {'bias': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       "    'kernel': array([[-0.03070462, -0.01288455, -0.08342619, ..., -0.04193836,\n",
       "             0.06740467, -0.06295494],\n",
       "           [-0.03129329,  0.02734923, -0.01014163, ...,  0.08416348,\n",
       "             0.06968652,  0.0266346 ],\n",
       "           [-0.05211964,  0.02943332, -0.08618319, ..., -0.07462934,\n",
       "            -0.06390709, -0.09353658],\n",
       "           ...,\n",
       "           [-0.03292365,  0.04858265,  0.04038563, ..., -0.000847  ,\n",
       "            -0.06037906,  0.10029769],\n",
       "           [-0.07978943, -0.03270318, -0.08589252, ...,  0.0387487 ,\n",
       "            -0.0110483 ,  0.04848849],\n",
       "           [-0.03012169, -0.01358447,  0.01814567, ..., -0.07275236,\n",
       "             0.05502048, -0.00230922]], dtype=float32)},\n",
       "   'ho': {'bias': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       "    'kernel': array([[ 0.07028317,  0.05542681,  0.03861254, ...,  0.03906544,\n",
       "            -0.04676572,  0.08407828],\n",
       "           [-0.02998612, -0.11939323, -0.08932067, ...,  0.01433718,\n",
       "             0.03216439, -0.02637681],\n",
       "           [ 0.07838727,  0.07762165, -0.01380515, ...,  0.03487621,\n",
       "            -0.07133428, -0.01153161],\n",
       "           ...,\n",
       "           [-0.05027618,  0.04835315,  0.02825768, ...,  0.08128677,\n",
       "             0.06131571, -0.01500933],\n",
       "           [-0.07470612,  0.01970041, -0.04772487, ...,  0.07952245,\n",
       "             0.01668522, -0.0630915 ],\n",
       "           [ 0.06373692, -0.1319161 , -0.00186018, ..., -0.04870198,\n",
       "            -0.01421127, -0.03175942]], dtype=float32)},\n",
       "   'if': {'kernel': array([[-0.06913762, -0.07059887, -0.04137419, ..., -0.02177091,\n",
       "            -0.04293613, -0.05085311],\n",
       "           [ 0.04163351,  0.01671084, -0.01369862, ..., -0.0610442 ,\n",
       "            -0.08513926,  0.07138945],\n",
       "           [-0.04776941,  0.05463628,  0.01001661, ...,  0.02562624,\n",
       "             0.012734  , -0.10704809],\n",
       "           ...,\n",
       "           [-0.03616921,  0.04173752, -0.06496057, ..., -0.01600539,\n",
       "            -0.12936892, -0.00504253],\n",
       "           [ 0.01423248, -0.05388161, -0.1087663 , ..., -0.03370342,\n",
       "             0.00879679,  0.03383046],\n",
       "           [-0.00996334,  0.11453044, -0.04316252, ...,  0.06015643,\n",
       "             0.02600653, -0.04071769]], dtype=float32)},\n",
       "   'ig': {'kernel': array([[-0.0402095 , -0.10076676,  0.10571638, ...,  0.01448792,\n",
       "            -0.07323707,  0.00997123],\n",
       "           [ 0.02581212,  0.0611636 , -0.07316866, ...,  0.06255464,\n",
       "             0.01694607,  0.01634375],\n",
       "           [ 0.03684547, -0.04380284, -0.02029416, ...,  0.00737481,\n",
       "            -0.00287163, -0.00458354],\n",
       "           ...,\n",
       "           [ 0.01529236,  0.04891386,  0.01907142, ..., -0.04614777,\n",
       "             0.05079086,  0.04883962],\n",
       "           [ 0.06325564,  0.03220044,  0.09710323, ...,  0.08539578,\n",
       "             0.00155484,  0.00452166],\n",
       "           [ 0.09667671,  0.00356998, -0.0504713 , ..., -0.11322274,\n",
       "             0.06378673,  0.12916999]], dtype=float32)},\n",
       "   'ii': {'kernel': array([[ 0.01129495, -0.05368592,  0.05019235, ...,  0.07618121,\n",
       "             0.00341828,  0.04302949],\n",
       "           [ 0.06823251, -0.07406103, -0.06480852, ..., -0.05092195,\n",
       "             0.01224094,  0.03626563],\n",
       "           [ 0.01109649, -0.05584716,  0.08611008, ...,  0.05936127,\n",
       "            -0.04478056, -0.04749877],\n",
       "           ...,\n",
       "           [-0.03941969,  0.06304143,  0.07030707, ...,  0.03390634,\n",
       "            -0.00927859,  0.07148732],\n",
       "           [ 0.07600643,  0.01005672,  0.00224798, ..., -0.04149346,\n",
       "            -0.07796965,  0.01096734],\n",
       "           [-0.01938727,  0.04340143,  0.11599219, ...,  0.10893753,\n",
       "            -0.10224798,  0.03235414]], dtype=float32)},\n",
       "   'io': {'kernel': array([[ 0.07965884, -0.06346702, -0.04666688, ...,  0.06753818,\n",
       "             0.00915976, -0.07453834],\n",
       "           [-0.12080175, -0.04537872,  0.0034946 , ..., -0.05723988,\n",
       "            -0.00138473,  0.12790814],\n",
       "           [-0.05993282, -0.0662539 , -0.01167892, ..., -0.00211636,\n",
       "            -0.03377046,  0.10930337],\n",
       "           ...,\n",
       "           [-0.03629902, -0.0671081 , -0.10099646, ..., -0.0265632 ,\n",
       "            -0.01792594,  0.04997274],\n",
       "           [ 0.03120087,  0.07594167, -0.04703434, ..., -0.00443388,\n",
       "             0.04405333,  0.07640827],\n",
       "           [-0.05034041,  0.04871321,  0.02940905, ...,  0.09128961,\n",
       "            -0.03953014,  0.07256761]], dtype=float32)}}}}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_ckpt[\"model\"][\"params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "066a5641-53c8-4262-bde9-2b7d707680f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "task.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa798df-4c3a-47e5-97f4-a47a38a2a661",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
