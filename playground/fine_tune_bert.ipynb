{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading, sit tight!\n",
      ">> Downloading review_polarity.tar.gz 100.06734377108491%%\n",
      "Successfully downloaded review_polarity.tar.gz 3127238 bytes\n",
      "\n",
      "Unzipping ...\n",
      "Deleting downloaded zip file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib\n",
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "\n",
    "corpus_url = \"http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\"\n",
    "\n",
    "corpus_root = \"/tmp/review_polarity/\"\n",
    "extracted_path = os.path.join(corpus_root, \"extracted\")\n",
    "if os.path.exists(corpus_root):\n",
    "    shutil.rmtree(corpus_root)\n",
    "if os.path.exists(extracted_path):\n",
    "    shutil.rmtree(extracted_path)\n",
    "\n",
    "os.makedirs(corpus_root)\n",
    "\n",
    "\n",
    "catgeories = [\"pos\", \"neg\"]\n",
    "\n",
    "\n",
    "def download_and_unzip():\n",
    "    file_name = corpus_url.split(\"/\")[-1]\n",
    "    download_path = os.path.join(corpus_root, file_name)\n",
    "\n",
    "    # ============================================ download\n",
    "    print(\"Downloading, sit tight!\")\n",
    "\n",
    "    def _progress(count, block_size, total_size):\n",
    "        sys.stdout.write(\n",
    "            f\"\\r>> Downloading {file_name} {float(count * block_size) / float(total_size) * 100.0}%\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    file_path, _ = urllib.request.urlretrieve(\n",
    "        corpus_url, download_path, _progress)\n",
    "    print()\n",
    "    print(\n",
    "        f\"Successfully downloaded {file_name} {os.stat(file_path).st_size} bytes\")\n",
    "\n",
    "    # ======================================= unzip\n",
    "    print()\n",
    "    print(\"Unzipping ...\")\n",
    "    # create dir at extracted_path\n",
    "    os.mkdir(extracted_path)\n",
    "    tarfile.open(file_path, \"r:gz\").extractall(extracted_path)\n",
    "\n",
    "    # =========================================== clean up\n",
    "    # delete the downloaded zip file\n",
    "    print(\"Deleting downloaded zip file\")\n",
    "    os.remove(file_path)\n",
    "\n",
    "\n",
    "# =============\n",
    "def read_text_files(path):\n",
    "    file_list = os.listdir(path)\n",
    "    texts = []\n",
    "\n",
    "    for fname in file_list:\n",
    "        fpath = os.path.join(path, fname)\n",
    "\n",
    "        f = open(fpath, mode=\"r\")\n",
    "        lines = f.read()\n",
    "        texts.append(lines)\n",
    "        f.close()\n",
    "\n",
    "    return texts\n",
    "\n",
    "\n",
    "# =========\n",
    "download_and_unzip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "836d7b9357d5426bb6864865d6018ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "prepare_corpus:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8e22cebff64fa78ccc4447a8e913e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "prepare_corpus:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "reviews = []\n",
    "labels = []\n",
    "\n",
    "# we can't use the previous tokenizers here\n",
    "# idx 0 -> neg, 1 -> pos\n",
    "for idx, cat in enumerate(catgeories):\n",
    "    path = os.path.join(extracted_path, \"txt_sentoken\", cat)\n",
    "    texts = read_text_files(path)\n",
    "\n",
    "    for i in tqdm(range(len(texts)), desc=\"prepare_corpus\"):\n",
    "        text = texts[i]\n",
    "        reviews.append(text)\n",
    "        labels.append(idx)\n",
    "\n",
    "print()\n",
    "print(len(reviews))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    reviews, labels, random_state=42, train_size=0.8\n",
    ")\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_train, y_train, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import FlaxAutoModel, AutoTokenizer\n",
    "import jax\n",
    "import flax.linen as nn\n",
    "import jax.numpy as np\n",
    "import numpy as onp\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': Array([[  101,  2023,  2003, 24369,  3793,   102]], dtype=int32), 'token_type_ids': Array([[0, 0, 0, 0, 0, 0]], dtype=int32), 'attention_mask': Array([[1, 1, 1, 1, 1, 1]], dtype=int32)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"google-bert/bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "clear_output()\n",
    "\n",
    "text = \"this is dummy text\"\n",
    "encoded = tokenizer.encode_plus(text, return_tensors=\"jax\")\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 512)\n",
      "(1, 512)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "class PolarityReviewDataset(Dataset):\n",
    "\n",
    "    def __init__(self, reviews, labels, tokenizer):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.reviews[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # encode review text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            return_token_type_ids=False,\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"np\"\n",
    "        )\n",
    "        \n",
    "        return encoding[\"input_ids\"], encoding[\"attention_mask\"], onp.array([label])\n",
    "\n",
    "\n",
    "training_dataset = PolarityReviewDataset(x_train, y_train, tokenizer)\n",
    "val_dataset = PolarityReviewDataset(x_val, y_val, tokenizer)\n",
    "test_dataset = PolarityReviewDataset(x_test, y_test, tokenizer)\n",
    "\n",
    "for ar in training_dataset[0]:\n",
    "    print(ar.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax_dataloader as jdl\n",
    "\n",
    "BS = 24\n",
    "\n",
    "train_loader = jdl.DataLoader(training_dataset, \"pytorch\", batch_size=BS, shuffle=True)\n",
    "val_loader = jdl.DataLoader(\n",
    "    val_dataset, \"pytorch\", batch_size=BS, shuffle=False)\n",
    "test_loader = jdl.DataLoader(test_dataset, \"pytorch\", batch_size=BS, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://flax.readthedocs.io/en/latest/guides/training_techniques/transfer_learning.html\n",
    "\n",
    "from typing import Any\n",
    "from flax.core.frozen_dict import unfreeze, freeze\n",
    "\n",
    "\n",
    "def load_model(model_name):\n",
    "    model = FlaxAutoModel.from_pretrained(model_name)\n",
    "    # clear_output()\n",
    "    module = model.module\n",
    "    variables = {\"params\": model.params}\n",
    "    return module, variables\n",
    "\n",
    "bert_module, bert_vars = load_model(model_name)\n",
    "\n",
    "\n",
    "class SentimentCLF(nn.Module):\n",
    "    backbone: nn.Module\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, input_ids: np.ndarray, attention_mask: np.ndarray) -> Any:\n",
    "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        out = out.pooler_output\n",
    "        out = nn.Dense(2)(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "rng = jax.random.key(0)\n",
    "model = SentimentCLF(bert_module)\n",
    "\n",
    "sample_data = training_dataset[0]\n",
    "input_ids, attention_mask, label = sample_data\n",
    "initial_params = model.init(rng, input_ids, attention_mask)\n",
    "\n",
    "# unfreeze\n",
    "initial_params_unfrozen = unfreeze(initial_params)\n",
    "initial_params_unfrozen[\"params\"][\"backbone\"] = bert_vars[\"params\"]\n",
    "# freeze back\n",
    "initial_params = freeze(initial_params_unfrozen)\n",
    "# initial_params = randomly_init_params[\"params\"]\n",
    "\n",
    "# # add pretrained vars\n",
    "# initial_params[\"backbone\"] = bert_vars[\"params\"]\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_loss=Array(1.075755, dtype=float32)\n",
      "batch_loss=Array(0.91052884, dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import optax\n",
    "\n",
    "@jax.jit\n",
    "def calculate_loss(params, input_ids, attention_mask, label):\n",
    "    logits = model.apply(params, input_ids, attention_mask)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, label)\n",
    "    # typical numpy array thing\n",
    "    # should be a scalar\n",
    "    return loss[0]\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def batched_loss(params, input_ids, attention_masks, labels):\n",
    "    batch_loss = jax.vmap(calculate_loss, in_axes=(None, 0, 0, 0))(params, input_ids, attention_masks, labels)\n",
    "    return batch_loss.mean(axis=-1)\n",
    "\n",
    "\n",
    "# =========\n",
    "single_loss = calculate_loss(initial_params, input_ids, attention_mask, label)\n",
    "print(f\"{single_loss=}\")\n",
    "\n",
    "for batch in train_loader:\n",
    "    input_ids, attention_masks, labels = batch\n",
    "    batch_loss = batched_loss(initial_params, input_ids, attention_masks, labels)\n",
    "    print(f\"{batch_loss=}\")\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training import train_state\n",
    "\n",
    "clipper = optax.clip_by_global_norm(1.0)\n",
    "\n",
    "tx = optax.chain(optax.adam(learning_rate=2e-5),\n",
    "                 optax.clip_by_global_norm(1.0))\n",
    "\n",
    "initial_state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    tx=tx,\n",
    "    params=initial_params,\n",
    ")\n",
    "criterion = jax.value_and_grad(batched_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e54f3f3dd44c48a07980a39ac0fc83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6617097785088094"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def test_step(state, batch):\n",
    "    input_ids, attention_masks, labels = batch\n",
    "\n",
    "    def infer(params, input_ids, attention_mask):\n",
    "        logits = model.apply(params, input_ids, attention_mask)\n",
    "        return jax.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    probas = jax.vmap(jax.jit(infer), in_axes=(None, 0, 0))(\n",
    "        state.params, input_ids, attention_masks)\n",
    "\n",
    "    return probas\n",
    "\n",
    "\n",
    "def evaluate(state, test_loader):\n",
    "    scores = list()\n",
    "    for batch in tqdm(test_loader):\n",
    "        _, _, labels = batch\n",
    "        probas = test_step(state, batch)\n",
    "        preds = onp.argmax(probas, axis=-1)\n",
    "        f1s = f1_score(labels, preds)\n",
    "\n",
    "        scores.append(f1s)\n",
    "\n",
    "    return onp.array(scores).mean(axis=-1)\n",
    "\n",
    "# ========== \n",
    "evaluate(initial_state, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    input_ids, attention_masks, labels = batch\n",
    "    loss_value, grads = criterion(state.params, input_ids, attention_masks, labels)    \n",
    "    updated_state = state.apply_gradients(grads=grads)\n",
    "    return loss_value, updated_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def validation_step(state, batch):\n",
    "    input_ids, attention_masks, labels = batch\n",
    "    loss_value, _ = criterion(state.params, input_ids, attention_masks, labels)\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4191169fc3f74fec8f64d0e1cfc7f6d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa60711a0284165b9aea00d1170a266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_step:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64f9aa188cb46f487e6cf0f534cbd06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation_step:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 :: Step : 40 :: Loss/Train : 0.6332657337188721 :: Loss/Validation : 0.4767797589302063\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d71f056e70f44159ea7b98c3e15bee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_step:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d524ba7e80a24edbb9a96603df9074de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation_step:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2 :: Step : 80 :: Loss/Train : 0.30096790194511414 :: Loss/Validation : 0.39539462327957153\n"
     ]
    }
   ],
   "source": [
    "def train(state, epochs, train_loader, val_loader):\n",
    "    steps = 0\n",
    "    train_losses = []\n",
    "    mean_val_losses = []\n",
    "\n",
    "\n",
    "    # =============\n",
    "    for e in trange(epochs):\n",
    "        for batch in tqdm(train_loader, desc=\"train_step\"):\n",
    "            train_loss, state = train_step(state, batch)\n",
    "            steps += 1\n",
    "\n",
    "            # log every 200 steps\n",
    "            if steps % 40 == 0:\n",
    "                train_losses.append(train_loss)\n",
    "\n",
    "                # run validation\n",
    "                validation_losses = []\n",
    "                for batch in tqdm(val_loader, desc=\"validation_step\"):\n",
    "                    val_loss = validation_step(state, batch)\n",
    "                    validation_losses.append(val_loss)\n",
    "                    \n",
    "                mean_val_loss = onp.array(validation_losses).mean(axis=-1)\n",
    "                mean_val_losses.append(mean_val_loss)\n",
    "\n",
    "\n",
    "                print(\n",
    "                    f\"Epoch : {e + 1} :: Step : {steps} :: Loss/Train : {train_loss} :: Loss/Validation : {mean_val_loss}\")\n",
    "                \n",
    "    # ============\n",
    "    return state, train_losses, mean_val_losses\n",
    "\n",
    "# ============\n",
    "trained_state, train_losses, mean_val_losses = train(initial_state, 2, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c1bec037b0d4a40a4c70b62ee09a8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.8723853634399746"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===========\n",
    "evaluate(trained_state, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
