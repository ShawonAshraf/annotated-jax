{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"batterydata/pos_tagging\"\n",
    "training_dataset = load_dataset(dataset_name, split=\"train\")\n",
    "test_dataset = load_dataset(dataset_name, split=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['words', 'labels'],\n",
       "    num_rows: 13054\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['words', 'labels'],\n",
       "    num_rows: 1451\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for data preprocessing:\n",
    "1. Make words to idx and labels to idx dictionaries\n",
    "2. Make a validation split from the training set\n",
    "3. Encode all the data with indices found from 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24848"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a dict containing word -> idx mapping\n",
    "def create_word_indices(dataset):\n",
    "    unique_words = set()\n",
    "    word_to_idx = dict()\n",
    "    # add an out of vocab token\n",
    "    oov_token = \"<OOV>\"\n",
    "    word_to_idx[oov_token] = 0\n",
    "    \n",
    "    # find unique words\n",
    "    for data in dataset:\n",
    "        words = data[\"words\"]\n",
    "        for w in words:\n",
    "            unique_words.add(w)\n",
    "            \n",
    "    # add index to them\n",
    "    for idx, uw in enumerate(list(unique_words)):\n",
    "        word_to_idx[uw] = idx + 1 # since oov is at 0\n",
    "        \n",
    "    \n",
    "    return word_to_idx\n",
    "\n",
    "\n",
    "# ===============\n",
    "word_to_idx = create_word_indices(training_dataset)\n",
    "len(word_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "def create_label_to_idx(dataset):\n",
    "    unique_labels = set()\n",
    "    label_to_idx = dict()\n",
    "    # add an out of vocab token\n",
    "    oov_token = \"<OOV>\"\n",
    "    label_to_idx[oov_token] = 0\n",
    "    \n",
    "    # find the labels\n",
    "    for data in dataset:\n",
    "        labels = data[\"labels\"]\n",
    "        for l in labels:\n",
    "            unique_labels.add(l)\n",
    "            \n",
    "    # index\n",
    "    for idx, label in enumerate(list(unique_labels)):\n",
    "        label_to_idx[label] = idx + 1\n",
    "        \n",
    "    return label_to_idx\n",
    "    \n",
    "label_to_idx = create_label_to_idx(training_dataset)\n",
    "print(len(label_to_idx))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 0, 'VBZ': 1, \"''\": 2, 'TO': 3, 'FW': 4, '-NONE-': 5, 'POS': 6, 'RBS': 7, 'UH': 8, 'LS': 9, 'RBR': 10, 'WP': 11, 'RP': 12, 'RB': 13, 'VB': 14, 'PRP$': 15, 'WRB': 16, 'CC': 17, 'WP$': 18, 'DT': 19, 'NN': 20, ')': 21, ',': 22, '``': 23, '$': 24, 'VBD': 25, 'VBG': 26, 'PRP': 27, 'SYM': 28, 'VBP': 29, '-RRB-': 30, 'IN': 31, '.': 32, 'VBN': 33, 'MD': 34, 'WDT': 35, '(': 36, 'JJS': 37, 'NNS': 38, '#': 39, 'JJR': 40, 'JJ': 41, ':': 42, 'CD': 43, 'NNP': 44, '-LRB-': 45, 'PDT': 46, 'NNPS': 47, 'EX': 48}\n"
     ]
    }
   ],
   "source": [
    "print(label_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'words': [972, 22062, 9313, 24281, 23983, 8078, 14643, 3389, 5765, 2188, 15739, 14561, 359, 3839, 18638, 24336, 19654, 222, 16838, 24336, 8019, 5675, 222, 3756, 3389, 17664, 6815, 14065, 2387, 13151, 23047, 10021, 22512, 19, 8031, 22436, 5927], 'labels': [20, 31, 19, 20, 1, 13, 33, 3, 14, 19, 41, 20, 31, 20, 38, 31, 44, 22, 41, 31, 20, 20, 22, 14, 3, 14, 19, 41, 20, 31, 44, 17, 44, 6, 41, 38, 32]}\n"
     ]
    }
   ],
   "source": [
    "# for a single instance\n",
    "def encode_data_instance(data, word_to_idx, label_to_idx):\n",
    "    words = [\n",
    "        word_to_idx[word] for word in data[\"words\"]\n",
    "    ]\n",
    "    \n",
    "    labels = [\n",
    "        label_to_idx[label] for label in data[\"labels\"]\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"words\": words,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "    \n",
    "\n",
    "print(encode_data_instance(training_dataset[0], word_to_idx, label_to_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'words': [972, 22062, 9313, 24281, 23983, 8078, 14643, 3389, 5765, 2188, 15739, 14561, 359, 3839, 18638, 24336, 19654, 222, 16838, 24336, 8019, 5675, 222, 3756, 3389, 17664, 6815, 14065, 2387, 13151, 23047, 10021, 22512, 19, 8031, 22436, 5927], 'labels': [20, 31, 19, 20, 1, 13, 33, 3, 14, 19, 41, 20, 31, 20, 38, 31, 44, 22, 41, 31, 20, 20, 22, 14, 3, 14, 19, 41, 20, 31, 44, 17, 44, 6, 41, 38, 32]}\n"
     ]
    }
   ],
   "source": [
    "trainset = map(lambda data: encode_data_instance(data, word_to_idx, label_to_idx), training_dataset)\n",
    "trainset = list(trainset)\n",
    "\n",
    "print(trainset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'words': [972, 22062, 9313, 24281, 23983, 8078, 14643, 3389, 5765, 2188, 15739, 14561, 359, 3839, 18638, 24336, 19654, 222, 16838, 24336, 8019, 5675, 222, 3756, 3389, 17664, 6815, 14065, 2387, 13151, 23047, 10021, 22512, 19, 8031, 22436, 5927], 'labels': [20, 31, 19, 20, 1, 13, 33, 3, 14, 19, 41, 20, 31, 20, 38, 31, 44, 22, 41, 31, 20, 20, 22, 14, 3, 14, 19, 41, 20, 31, 44, 17, 44, 6, 41, 38, 32]}\n"
     ]
    }
   ],
   "source": [
    "testset = map(lambda data: encode_data_instance(\n",
    "    data, word_to_idx, label_to_idx), test_dataset)\n",
    "testset = list(testset)\n",
    "\n",
    "print(testset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(training_dataset) == len(trainset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9138\n",
      "3916\n"
     ]
    }
   ],
   "source": [
    "# now to create the validation set\n",
    "import numpy as np\n",
    "\n",
    "def create_train_validation_splits(trainset, validation_ratio):\n",
    "    validation_set_size = int(len(trainset) * validation_ratio)\n",
    "    validation_indices = np.random.choice(len(trainset), replace=False, size=validation_set_size).tolist()\n",
    "    \n",
    "    # now to separate trainset indices\n",
    "    trainset_indices = [i for i in range(len(trainset)) if i not in validation_indices]\n",
    "    \n",
    "    return trainset_indices, validation_indices\n",
    "\n",
    "\n",
    "trainset_indices, validation_indices = create_train_validation_splits(trainset, 0.3)\n",
    "\n",
    "print(len(trainset_indices))\n",
    "print(len(validation_indices))\n",
    "\n",
    "\n",
    "assert len(trainset_indices) + len(validation_indices) == len(trainset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random, jit, vmap, grad\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "\n",
    "\n",
    "master_key = random.PRNGKey(seed=2023)\n",
    "master_key, model_init_key = random.split(master_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LSTMCell.__init__() got an unexpected keyword argument 'features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/shawon/Projects/jax_examples/playground/pos_tagging_lstm.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnot-real-slim-shady/home/shawon/Projects/jax_examples/playground/pos_tagging_lstm.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m x\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnot-real-slim-shady/home/shawon/Projects/jax_examples/playground/pos_tagging_lstm.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m model \u001b[39m=\u001b[39m LSTMTagger(\u001b[39mlen\u001b[39m(word_to_idx), \u001b[39m150\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39mlen\u001b[39m(label_to_idx))\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bnot-real-slim-shady/home/shawon/Projects/jax_examples/playground/pos_tagging_lstm.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m init_params \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49minit(model_init_key, np\u001b[39m.\u001b[39;49marray(trainset[\u001b[39m0\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mwords\u001b[39;49m\u001b[39m\"\u001b[39;49m]))\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "\u001b[1;32m/home/shawon/Projects/jax_examples/playground/pos_tagging_lstm.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnot-real-slim-shady/home/shawon/Projects/jax_examples/playground/pos_tagging_lstm.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m@nn\u001b[39m\u001b[39m.\u001b[39mcompact\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnot-real-slim-shady/home/shawon/Projects/jax_examples/playground/pos_tagging_lstm.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, words) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnot-real-slim-shady/home/shawon/Projects/jax_examples/playground/pos_tagging_lstm.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbed(num_embeddings\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_size, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_dimensions)(words)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bnot-real-slim-shady/home/shawon/Projects/jax_examples/playground/pos_tagging_lstm.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mLSTMCell(features\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprojection_dims)(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnot-real-slim-shady/home/shawon/Projects/jax_examples/playground/pos_tagging_lstm.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDense(features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_labels)(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnot-real-slim-shady/home/shawon/Projects/jax_examples/playground/pos_tagging_lstm.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mleaky_relu(x)\n",
      "\u001b[0;31mTypeError\u001b[0m: LSTMCell.__init__() got an unexpected keyword argument 'features'"
     ]
    }
   ],
   "source": [
    "# lstm in flax: https://flax.readthedocs.io/en/latest/api_reference/flax.linen/_autosummary/flax.linen.LSTMCell.html\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "    vocab_size: int\n",
    "    embedding_dimensions: int\n",
    "    projection_dims: int # aka hidden dims for projection after lstm\n",
    "    n_labels: int\n",
    "    \n",
    "    def setup(self) -> None:\n",
    "        self.embedding = nn.Embed(\n",
    "            num_embeddings=self.vocab_size, features=self.embedding_dimensions)\n",
    "        \n",
    "        self.lstm = nn.LSTMCell(self.projection_dims)\n",
    "        self.dense = nn.Dense(features=self.n_labels)\n",
    "        \n",
    "    \n",
    "    def __call__(self, words) -> Any:\n",
    "        x = (words)\n",
    "        x = nn.LSTMCell(features=self.projection_dims)(x)\n",
    "        x = nn.Dense(features=self.n_labels)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        x = nn.log_softmax(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "model = LSTMTagger(len(word_to_idx), 150, 100, len(label_to_idx))\n",
    "\n",
    "init_params = model.init(model_init_key, np.array(trainset[0][\"words\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    params: {\n",
       "        Embed_0: {\n",
       "            embedding: Array([[-0.05916596,  0.01026116,  0.08525214, ..., -0.03253829,\n",
       "                     0.01560668, -0.0402378 ],\n",
       "                   [ 0.08890485, -0.09597716,  0.01476798, ...,  0.26407877,\n",
       "                     0.10020454,  0.06108525],\n",
       "                   [ 0.01983153, -0.04647755, -0.03345514, ...,  0.05045682,\n",
       "                     0.06162264,  0.03780266],\n",
       "                   ...,\n",
       "                   [ 0.00149957, -0.13568471, -0.03070986, ..., -0.1231816 ,\n",
       "                    -0.07690515,  0.0227146 ],\n",
       "                   [-0.01137438, -0.09372773,  0.03765107, ...,  0.04488415,\n",
       "                     0.09964965, -0.01214164],\n",
       "                   [ 0.0132378 , -0.09923965, -0.0267396 , ...,  0.06017342,\n",
       "                    -0.07800906,  0.08927516]], dtype=float32),\n",
       "        },\n",
       "        Dense_0: {\n",
       "            kernel: Array([[-0.03942307, -0.12093627, -0.07201098, ..., -0.1012962 ,\n",
       "                     0.02250638,  0.05820093],\n",
       "                   [-0.06712638, -0.01199719, -0.13874733, ...,  0.09258776,\n",
       "                    -0.04519978,  0.12671845],\n",
       "                   [-0.07802419,  0.10123412, -0.03258402, ...,  0.05636023,\n",
       "                     0.12783283, -0.02566418],\n",
       "                   ...,\n",
       "                   [-0.01788905,  0.0362769 , -0.10749218, ..., -0.10193478,\n",
       "                    -0.03041202,  0.04452768],\n",
       "                   [-0.0640425 ,  0.08807496,  0.03709094, ..., -0.09584117,\n",
       "                    -0.09030198, -0.08536918],\n",
       "                   [ 0.02540789, -0.05466649, -0.01666202, ...,  0.03805858,\n",
       "                     0.06624938, -0.05436395]], dtype=float32),\n",
       "            bias: Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                   0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                   0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],      dtype=float32),\n",
       "        },\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_examples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
