{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"batterydata/pos_tagging\"\n",
    "training_dataset = load_dataset(dataset_name, split=\"train\")\n",
    "test_dataset = load_dataset(dataset_name, split=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for data preprocessing:\n",
    "1. Make words to idx and labels to idx dictionaries\n",
    "2. Make a validation split from the training set\n",
    "3. Encode all the data with indices found from 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dict containing word -> idx mapping\n",
    "def create_word_indices(dataset):\n",
    "    unique_words = set()\n",
    "    word_to_idx = dict()\n",
    "    # add an out of vocab token\n",
    "    oov_token = \"<OOV>\"\n",
    "    pad_token = \"<PAD>\"\n",
    "    word_to_idx[oov_token] = 0\n",
    "    word_to_idx[pad_token] = 1\n",
    "    \n",
    "    # find unique words\n",
    "    for data in dataset:\n",
    "        words = data[\"words\"]\n",
    "        for w in words:\n",
    "            unique_words.add(w)\n",
    "            \n",
    "    # add index to them\n",
    "    for idx, uw in enumerate(list(unique_words)):\n",
    "        word_to_idx[uw] = idx + 2 # since oov is at 0 and pad at 1\n",
    "        \n",
    "    \n",
    "    return word_to_idx\n",
    "\n",
    "\n",
    "# ===============\n",
    "word_to_idx = create_word_indices(training_dataset)\n",
    "len(word_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_to_idx(dataset):\n",
    "    unique_labels = set()\n",
    "    label_to_idx = dict()\n",
    "    # add an out of vocab token\n",
    "    oov_token = \"<OOV>\"\n",
    "    pad_token = \"<PAD>\"\n",
    "    label_to_idx[oov_token] = 0\n",
    "    label_to_idx[pad_token] = 1\n",
    "    \n",
    "    # find the labels\n",
    "    for data in dataset:\n",
    "        labels = data[\"labels\"]\n",
    "        for l in labels:\n",
    "            unique_labels.add(l)\n",
    "            \n",
    "    # index\n",
    "    for idx, label in enumerate(list(unique_labels)):\n",
    "        label_to_idx[label] = idx + 2\n",
    "        \n",
    "    return label_to_idx\n",
    "    \n",
    "label_to_idx = create_label_to_idx(training_dataset)\n",
    "print(len(label_to_idx))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a single instance\n",
    "def encode_data_instance(data, word_to_idx, label_to_idx):\n",
    "    words = [\n",
    "        word_to_idx.get(word, word_to_idx[\"<OOV>\"]) for word in data[\"words\"]\n",
    "    ]\n",
    "    \n",
    "    labels = [\n",
    "        label_to_idx[label] for label in data[\"labels\"]\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"words\": words,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "    \n",
    "\n",
    "print(encode_data_instance(training_dataset[0], word_to_idx, label_to_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = map(lambda data: encode_data_instance(data, word_to_idx, label_to_idx), training_dataset)\n",
    "trainset = list(trainset)\n",
    "\n",
    "print(trainset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = map(lambda data: encode_data_instance(\n",
    "    data, word_to_idx, label_to_idx), test_dataset)\n",
    "testset = list(testset)\n",
    "\n",
    "print(testset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(training_dataset) == len(trainset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now to create the validation set\n",
    "import numpy as np\n",
    "\n",
    "def create_train_validation_splits(trainset, validation_ratio):\n",
    "    validation_set_size = int(len(trainset) * validation_ratio)\n",
    "    validation_indices = np.random.choice(len(trainset), replace=False, size=validation_set_size).tolist()\n",
    "    \n",
    "    # now to separate trainset indices\n",
    "    trainset_indices = [i for i in range(len(trainset)) if i not in validation_indices]\n",
    "    \n",
    "    return trainset_indices, validation_indices\n",
    "\n",
    "\n",
    "trainset_indices, validation_indices = create_train_validation_splits(trainset, 0.3)\n",
    "\n",
    "print(len(trainset_indices))\n",
    "print(len(validation_indices))\n",
    "\n",
    "\n",
    "assert len(trainset_indices) + len(validation_indices) == len(trainset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = np.max([len(d[\"words\"]) for d in trainset])\n",
    "max_seq_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import jax_dataloader as jdl\n",
    "\n",
    "class TagDataset(Dataset):\n",
    "    def __init__(self, indices, dataset) -> None:\n",
    "        self.indices = indices\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.indices is None:\n",
    "            # this is for the test case\n",
    "            return len(self.dataset)\n",
    "        else:\n",
    "            return len(self.indices)\n",
    "        \n",
    "    def __getitem__(self, index) -> np.ndarray:\n",
    "        if self.indices is None:\n",
    "            idx = index\n",
    "        else:\n",
    "            idx = self.indices[index]\n",
    "            \n",
    "        data = self.dataset[idx]\n",
    "        \n",
    "        # padding to 300\n",
    "        # pad token idx is 1\n",
    "        words = np.ones((300, ), dtype=np.int32)\n",
    "        words[:len(data[\"words\"])] = data[\"words\"] \n",
    "    \n",
    "        \n",
    "        labels = np.ones((300, ), dtype=np.int32)\n",
    "        labels[:len(data[\"labels\"])] = data[\"labels\"]\n",
    "        \n",
    "        # labels = np.array(data[\"labels\"])\n",
    "        \n",
    "        return words, labels\n",
    "\n",
    "train_loader = jdl.DataLoader(TagDataset(trainset_indices, trainset), \"pytorch\", batch_size=128, shuffle=True)\n",
    "val_loader = jdl.DataLoader(TagDataset(validation_indices, trainset), \"pytorch\", batch_size=128, shuffle=False)\n",
    "train_loader = jdl.DataLoader(TagDataset(None, testset), \"pytorch\", batch_size=128, shuffle=False)\n",
    "\n",
    "\n",
    "# =========== test a dataloader ==========\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random, jit, vmap, grad, value_and_grad\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_key = random.PRNGKey(seed=2023)\n",
    "master_key, model_init_key = random.split(master_key)\n",
    "master_key, dropout_key = random.split(master_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm in flax: https://flax.readthedocs.io/en/latest/api_reference/flax.linen/_autosummary/flax.linen.LSTMCell.html\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "    vocab_size: int\n",
    "    embedding_dimensions: int\n",
    "    projection_dims: int # aka hidden dims for projection after lstm\n",
    "    n_labels: int\n",
    "    training = True\n",
    "        \n",
    "    @nn.compact\n",
    "    def __call__(self, words) -> Any:        \n",
    "        # ========== Embedding ==========\n",
    "        x = nn.Embed(\n",
    "            num_embeddings=self.vocab_size, features=self.embedding_dimensions, name=\"embedding\")(words)\n",
    "        x = nn.Dropout(0.2, deterministic=not self.training)(x)\n",
    "        \n",
    "        \n",
    "        # ========= LSTM ============\n",
    "        lstm = nn.OptimizedLSTMCell(features=self.projection_dims, name=\"lstm\")\n",
    "        carry = lstm.initialize_carry(random.PRNGKey(2024), x.shape)    \n",
    "        carry, x = lstm(carry=carry, inputs=x)\n",
    "        \n",
    "        # ========== Dense ==========\n",
    "        x = nn.Dense(features=self.n_labels, name=\"dense\")(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "model = LSTMTagger(len(word_to_idx), 300, 300, 300)\n",
    "\n",
    "# why ?\n",
    "# https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html\n",
    "model_rngs = {\"params\": model_init_key, \"dropout\": dropout_key}\n",
    "\n",
    "init_params = model.init(model_rngs, np.array(trainset[0][\"words\"]))\n",
    "logits = model.apply(init_params, jnp.array(trainset[0][\"words\"]), rngs={\"dropout\": random.PRNGKey(99)})\n",
    "print(logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "\n",
    "@jit\n",
    "def calculate_loss(params, words, labels):\n",
    "    logits = model.apply(params, words, rngs={\"dropout\": random.PRNGKey(90)})\n",
    "    loss = optax.softmax_cross_entropy(logits, labels)\n",
    "    return loss.mean(axis=-1)\n",
    "\n",
    "\n",
    "# you're vmapping the whole grad function!\n",
    "# have a separate batch loss function!\n",
    "@jit\n",
    "def batched_loss(params, words_batched, labels_batched):\n",
    "    batch_loss = vmap(calculate_loss, in_axes=(None, 0, 0))(params, words_batched, labels_batched)\n",
    "    return batch_loss.mean(axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import trange\n",
    "from flax.training import train_state\n",
    "from functools import partial\n",
    "\n",
    "optimiser = optax.sgd(learning_rate=0.01)\n",
    "init_state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply, # the forward function\n",
    "    params=init_params,\n",
    "    tx=optimiser\n",
    ")\n",
    "criterion = value_and_grad(batched_loss)\n",
    "\n",
    "\n",
    "\n",
    "@partial(jit, static_argnums=0)\n",
    "def train_step(criterion, state, words_batched, labels_batched):\n",
    "    loss_value, grads = criterion(state.params, words_batched, labels_batched)    \n",
    "    updated_state = state.apply_gradients(grads=grads)\n",
    "    return loss_value, updated_state\n",
    "\n",
    "\n",
    "def train_model(state, train_loader, epochs=100, log_every_n_step=200):    \n",
    "    step_counter = 0\n",
    "    for _ in trange(epochs):\n",
    "        for batch in train_loader:\n",
    "            words, labels = batch\n",
    "            loss, state = train_step(criterion, state, words, labels)\n",
    "            \n",
    "            \n",
    "            step_counter += 1\n",
    "            if step_counter % log_every_n_step == 0:\n",
    "                print(loss)\n",
    "\n",
    "    return state \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = train_model(init_state, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_examples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
