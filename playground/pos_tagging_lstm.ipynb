{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"batterydata/pos_tagging\"\n",
    "training_dataset = load_dataset(dataset_name, split=\"train\")\n",
    "test_dataset = load_dataset(dataset_name, split=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['words', 'labels'],\n",
       "    num_rows: 13054\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['words', 'labels'],\n",
       "    num_rows: 1451\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for data preprocessing:\n",
    "1. Make words to idx and labels to idx dictionaries\n",
    "2. Make a validation split from the training set\n",
    "3. Encode all the data with indices found from 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24848"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a dict containing word -> idx mapping\n",
    "def create_word_indices(dataset):\n",
    "    unique_words = set()\n",
    "    word_to_idx = dict()\n",
    "    # add an out of vocab token\n",
    "    oov_token = \"<OOV>\"\n",
    "    word_to_idx[oov_token] = 0\n",
    "    \n",
    "    # find unique words\n",
    "    for data in dataset:\n",
    "        words = data[\"words\"]\n",
    "        for w in words:\n",
    "            unique_words.add(w)\n",
    "            \n",
    "    # add index to them\n",
    "    for idx, uw in enumerate(list(unique_words)):\n",
    "        word_to_idx[uw] = idx + 1 # since oov is at 0\n",
    "        \n",
    "    \n",
    "    return word_to_idx\n",
    "\n",
    "\n",
    "# ===============\n",
    "word_to_idx = create_word_indices(training_dataset)\n",
    "len(word_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    }
   ],
   "source": [
    "def create_label_to_idx(dataset):\n",
    "    unique_labels = set()\n",
    "    label_to_idx = dict()\n",
    "    # add an out of vocab token\n",
    "    oov_token = \"<OOV>\"\n",
    "    label_to_idx[oov_token] = 0\n",
    "    \n",
    "    # find the labels\n",
    "    for data in dataset:\n",
    "        labels = data[\"labels\"]\n",
    "        for l in labels:\n",
    "            unique_labels.add(l)\n",
    "            \n",
    "    # index\n",
    "    for idx, label in enumerate(list(unique_labels)):\n",
    "        label_to_idx[label] = idx + 1\n",
    "        \n",
    "    return label_to_idx\n",
    "    \n",
    "label_to_idx = create_label_to_idx(training_dataset)\n",
    "print(len(label_to_idx))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 0, '-RRB-': 1, 'CD': 2, '(': 3, 'NN': 4, 'VBG': 5, 'VBZ': 6, 'RBS': 7, 'PRP': 8, 'EX': 9, 'LS': 10, 'JJ': 11, 'PRP$': 12, 'WRB': 13, 'RP': 14, 'DT': 15, 'WP': 16, '-NONE-': 17, 'RB': 18, '-LRB-': 19, 'NNP': 20, 'JJR': 21, ':': 22, ')': 23, 'MD': 24, 'WDT': 25, '#': 26, ',': 27, 'TO': 28, 'UH': 29, 'FW': 30, 'VBP': 31, 'NNPS': 32, 'PDT': 33, 'NNS': 34, 'VBN': 35, 'CC': 36, '``': 37, 'RBR': 38, 'VBD': 39, 'IN': 40, \"''\": 41, '$': 42, 'SYM': 43, 'JJS': 44, 'WP$': 45, 'POS': 46, 'VB': 47, '.': 48}\n"
     ]
    }
   ],
   "source": [
    "print(label_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'words': [15446, 427, 22449, 8706, 2829, 2527, 16321, 19167, 5695, 20770, 24005, 10805, 14760, 11868, 19064, 2835, 23737, 12453, 19839, 2835, 18153, 12428, 12453, 5759, 19167, 19285, 11537, 20158, 2837, 18421, 15437, 2231, 13408, 13704, 3811, 12177, 8760], 'labels': [4, 40, 15, 4, 6, 18, 35, 28, 47, 15, 11, 4, 40, 4, 34, 40, 20, 27, 11, 40, 4, 4, 27, 47, 28, 47, 15, 11, 4, 40, 20, 36, 20, 46, 11, 34, 48]}\n"
     ]
    }
   ],
   "source": [
    "# for a single instance\n",
    "def encode_data_instance(data, word_to_idx, label_to_idx):\n",
    "    words = [\n",
    "        word_to_idx.get(word, word_to_idx[\"<OOV>\"]) for word in data[\"words\"]\n",
    "    ]\n",
    "    \n",
    "    labels = [\n",
    "        label_to_idx[label] for label in data[\"labels\"]\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"words\": words,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "    \n",
    "\n",
    "print(encode_data_instance(training_dataset[0], word_to_idx, label_to_idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'words': [15446, 427, 22449, 8706, 2829, 2527, 16321, 19167, 5695, 20770, 24005, 10805, 14760, 11868, 19064, 2835, 23737, 12453, 19839, 2835, 18153, 12428, 12453, 5759, 19167, 19285, 11537, 20158, 2837, 18421, 15437, 2231, 13408, 13704, 3811, 12177, 8760], 'labels': [4, 40, 15, 4, 6, 18, 35, 28, 47, 15, 11, 4, 40, 4, 34, 40, 20, 27, 11, 40, 4, 4, 27, 47, 28, 47, 15, 11, 4, 40, 20, 36, 20, 46, 11, 34, 48]}\n"
     ]
    }
   ],
   "source": [
    "trainset = map(lambda data: encode_data_instance(data, word_to_idx, label_to_idx), training_dataset)\n",
    "trainset = list(trainset)\n",
    "\n",
    "print(trainset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'words': [7390, 0, 21970, 22449, 2671, 5809, 12453, 21448, 16107, 9733, 10171, 12453, 6045, 22449, 4621, 20680, 3481, 6866, 5509, 15700, 427, 19232, 6218, 11362, 12453, 2231, 7783, 22063, 18696, 19, 19924, 11757, 10383, 8760], 'labels': [36, 20, 39, 15, 20, 4, 27, 8, 6, 17, 17, 27, 40, 15, 20, 20, 11, 4, 6, 4, 40, 11, 40, 4, 27, 36, 18, 6, 18, 47, 15, 4, 34, 48]}\n"
     ]
    }
   ],
   "source": [
    "testset = map(lambda data: encode_data_instance(\n",
    "    data, word_to_idx, label_to_idx), test_dataset)\n",
    "testset = list(testset)\n",
    "\n",
    "print(testset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(training_dataset) == len(trainset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9138\n",
      "3916\n"
     ]
    }
   ],
   "source": [
    "# now to create the validation set\n",
    "import numpy as np\n",
    "\n",
    "def create_train_validation_splits(trainset, validation_ratio):\n",
    "    validation_set_size = int(len(trainset) * validation_ratio)\n",
    "    validation_indices = np.random.choice(len(trainset), replace=False, size=validation_set_size).tolist()\n",
    "    \n",
    "    # now to separate trainset indices\n",
    "    trainset_indices = [i for i in range(len(trainset)) if i not in validation_indices]\n",
    "    \n",
    "    return trainset_indices, validation_indices\n",
    "\n",
    "\n",
    "trainset_indices, validation_indices = create_train_validation_splits(trainset, 0.3)\n",
    "\n",
    "print(len(trainset_indices))\n",
    "print(len(validation_indices))\n",
    "\n",
    "\n",
    "assert len(trainset_indices) + len(validation_indices) == len(trainset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random, jit, vmap, grad\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_key = random.PRNGKey(seed=2023)\n",
    "master_key, model_init_key = random.split(master_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm in flax: https://flax.readthedocs.io/en/latest/api_reference/flax.linen/_autosummary/flax.linen.LSTMCell.html\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "    vocab_size: int\n",
    "    embedding_dimensions: int\n",
    "    projection_dims: int # aka hidden dims for projection after lstm\n",
    "    n_labels: int\n",
    "        \n",
    "    @nn.compact\n",
    "    def __call__(self, words) -> Any:\n",
    "        # ========== Embedding ==========\n",
    "        x = nn.Embed(\n",
    "            num_embeddings=self.vocab_size, features=self.embedding_dimensions)(words)\n",
    "        \n",
    "        # ========= LSTM ============\n",
    "        lstm = nn.OptimizedLSTMCell(features=self.projection_dims)\n",
    "        carry = lstm.initialize_carry(random.PRNGKey(2023), x.shape)    \n",
    "        carry, x = lstm(carry=carry, inputs=x)\n",
    "        \n",
    "        # ========== Dense ==========\n",
    "        x = nn.Dense(features=self.n_labels)(x)\n",
    "        x = nn.leaky_relu(x)\n",
    "        \n",
    "        x = nn.log_softmax(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "model = LSTMTagger(len(word_to_idx), 150, 150, len(label_to_idx))\n",
    "\n",
    "init_params = model.init(model_init_key, np.array(trainset[0][\"words\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_examples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
