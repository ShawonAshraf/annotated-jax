{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib\n",
    "import tarfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_url = \"http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\"\n",
    "\n",
    "corpus_root = os.path.join(os.getcwd(), \"review_polarity\", \"txt_sentoken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download corpus as a zip and then unzip\n",
    "# downloads and unzips in the same directory\n",
    "# by default set to current dir\n",
    "def download_and_unzip():\n",
    "    file_name = corpus_url.split(\"/\")[-1]\n",
    "    download_path = os.path.join(os.getcwd(), file_name)\n",
    "    # where the zip will get extracted\n",
    "    extracted_path = os.path.join(os.getcwd(), \"review_polarity\")\n",
    "\n",
    "    if os.path.exists(extracted_path):\n",
    "        print(\"Already downloaded and extracted!\")\n",
    "    else:\n",
    "        # ============================================ download\n",
    "        print(\"Downloading, sit tight!\")\n",
    "\n",
    "        def _progress(count, block_size, total_size):\n",
    "            sys.stdout.write(\n",
    "                f\"\\r>> Downloading {file_name} {float(count * block_size) / float(total_size) * 100.0}%\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        file_path, _ = urllib.request.urlretrieve(\n",
    "            corpus_url, download_path, _progress)\n",
    "        print()\n",
    "        print(\n",
    "            f\"Successfully downloaded {file_name} {os.stat(file_path).st_size} bytes\")\n",
    "\n",
    "        # ======================================= unzip\n",
    "        print()\n",
    "        print(\"Unzipping ...\")\n",
    "        # create dir at extracted_path\n",
    "        os.mkdir(extracted_path)\n",
    "        tarfile.open(file_path, \"r:gz\").extractall(extracted_path)\n",
    "\n",
    "        # =========================================== clean up\n",
    "        # delete the downloaded zip file\n",
    "        print(\"Deleting downloaded zip file\")\n",
    "        os.remove(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already downloaded and extracted!\n"
     ]
    }
   ],
   "source": [
    "download_and_unzip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Review:\n",
    "    tokens: List[str]\n",
    "    label: int # 1 if pos else 0\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return str(self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from nltk.tokenize import word_tokenize as tokenize\n",
    "\n",
    "# just read all files for a category\n",
    "\n",
    "\n",
    "def load_data_from_path(path) -> List[Review]:\n",
    "    file_list = os.listdir(path)\n",
    "    data: List[Review] = []\n",
    "\n",
    "    for _, fname in tqdm(enumerate(file_list)):\n",
    "        fpath = os.path.join(path, fname)\n",
    "\n",
    "        # read text from the file\n",
    "        f = open(fpath, mode=\"r\")\n",
    "        lines = f.read()\n",
    "        # close\n",
    "        f.close()\n",
    "        \n",
    "        # tokenize\n",
    "        tokens = tokenize(lines)\n",
    "        \n",
    "        # create review object\n",
    "        review = Review(tokens, 1 if \"pos\" in path else 0)\n",
    "        \n",
    "        # add to the data list\n",
    "        data.append(review)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940a972356584be78b6da017fc8122a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00904e34d0c443e0823cb3326c38fdaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos = load_data_from_path(\n",
    "    \"./review_polarity/txt_sentoken/pos\")\n",
    "neg = load_data_from_path(\"./review_polarity/txt_sentoken/neg\")\n",
    "\n",
    "# combine into a single list\n",
    "all_data = pos + neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafka/miniconda3/envs/jax_examples/lib/python3.10/site-packages/gensim/models/keyedvectors.py:551: UserWarning: Adding single vectors to a KeyedVectors which grows by one each time can be costly. Consider adding in batches or preallocating to the required size.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1193515"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "import numpy as np\n",
    "\n",
    "glove_vectors = gensim.downloader.load('glove-twitter-200')\n",
    "\n",
    "shape = glove_vectors[\"good\"].shape\n",
    "unk = np.zeros(shape=shape)\n",
    "\n",
    "glove_vectors.add_vector(\"<UNK>\", unk)\n",
    "glove_vectors.add_vector(\"<PAD>\", np.ones(shape=shape) * -1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2753"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the max sequence length\n",
    "max_seq_len = max([len(d.tokens) for d in all_data])\n",
    "max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240f9f5a9b304b619dfeebd46bcdf2b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@dataclass\n",
    "class EncodedData:\n",
    "    tokens: np.ndarray\n",
    "    label: int\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return str(self.__dict__)\n",
    "\n",
    "\n",
    "# encode with indexes from the word vectors\n",
    "# also pad\n",
    "def encode_pad_text(data: List[Review]=all_data, pad_len=max_seq_len) -> List[EncodedData]:\n",
    "    encoded: List[EncodedData] = []\n",
    "   \n",
    "    # encode\n",
    "    for _, review in tqdm(enumerate(data)):\n",
    "        tokens = review.tokens\n",
    "        indexes = list()\n",
    "       \n",
    "        for tok in tokens:\n",
    "            try:\n",
    "                idx = glove_vectors.key_to_index[tok]\n",
    "            except:\n",
    "                idx = glove_vectors.key_to_index[\"<UNK>\"]\n",
    "\n",
    "            indexes.append(idx)\n",
    "    \n",
    "        # pad\n",
    "        indexes = np.array(indexes)\n",
    "        padded = np.ones(shape=(pad_len, ), dtype=np.int32) * \\\n",
    "            glove_vectors.key_to_index[\"<PAD>\"]\n",
    "        # insert indexes to padded\n",
    "        padded[:indexes.shape[0]] = indexes\n",
    "        \n",
    "        # create an encoded data object\n",
    "        enc = EncodedData(padded, review.label)\n",
    "        encoded.append(enc)\n",
    "        \n",
    "   \n",
    "    return encoded\n",
    "\n",
    "\n",
    "encoded_data = encode_pad_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncodedData(tokens=array([   6866,  111052,     133, ..., 1193515, 1193515, 1193515],\n",
       "      dtype=int32), label=1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(encoded_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "\n",
    "assert len(train_data) + len(test_data) == len(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    kernel_sizes = [3, 4, 5]\n",
    "    n_filters = 100\n",
    "    embedding_dim = 200\n",
    "    out_dim = 2\n",
    "    \n",
    "    def setup(self):\n",
    "        self.conv_layer = [\n",
    "            nn.Conv(features=self.n_filters, kernel_size=ksize) for ksize in self.kernel_sizes\n",
    "        ]\n",
    "        \n",
    "        self.embedding = nn.Embed(features=200, \n",
    "                                  embedding=glove_vectors, \n",
    "                                  num_embeddings=len(glove_vectors.index_to_key))\n",
    "        \n",
    "        self.dense = nn.Dense(features=self.out_dim)\n",
    "        \n",
    "    def conv_and_pool(self, x, conv):\n",
    "        out = nn.relu(conv(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_examples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2ea88ac40e8e5d58ad2f5d32fe0babb793b131664cfdc2d06961b2c577b6c457"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
