{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with ANN\n",
    "\n",
    "So far, you've seen how to define a model using jax, train the model, do backpropagation, and test it. Let's take it a bit further. In this notebook we'll be writing a simple ANN to classify penguin species using the [palmers penguin dataset](https://github.com/mcnakhaee/palmerpenguins).\n",
    "\n",
    "Let's load the data and process it to get started. (apparently the main focus of this notebook is to show you the ANN and not how you can load data, so, I'm going to short circuit the whole process a bit)\n",
    "\n",
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from palmerpenguins import load_penguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
      "0  Adelie  Torgersen            39.1           18.7              181.0   \n",
      "1  Adelie  Torgersen            39.5           17.4              186.0   \n",
      "2  Adelie  Torgersen            40.3           18.0              195.0   \n",
      "4  Adelie  Torgersen            36.7           19.3              193.0   \n",
      "5  Adelie  Torgersen            39.3           20.6              190.0   \n",
      "\n",
      "   body_mass_g     sex  year  \n",
      "0       3750.0    male  2007  \n",
      "1       3800.0  female  2007  \n",
      "2       3250.0  female  2007  \n",
      "4       3450.0  female  2007  \n",
      "5       3650.0    male  2007  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2129ee5e39e9462a88c4d6ea2f2638b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(333, 4)\n",
      "(333,)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    penguins = load_penguins() # penguins is a dataframe\n",
    "    penguins = penguins.dropna() # type: ignore\n",
    "    \n",
    "    # print the head of the dataframe to give some view\n",
    "    print(penguins.head()) # type: ignore\n",
    "    \n",
    "    # collect the feature columns\n",
    "    feature_columns = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
    "    # classification target\n",
    "    target_column = \"species\"\n",
    "    \n",
    "    # features and targets\n",
    "    features = penguins[feature_columns].values  # type: ignore\n",
    "    targets = penguins[target_column].values  # type: ignore\n",
    "    \n",
    "    # but here's a catch\n",
    "    # the targets are categorical, so we have two two options here\n",
    "    # one hot encode them, or, assign a numeric value to them and keep a dictionary\n",
    "    # with the target label to int mapping\n",
    "    # the second approach is easier xD\n",
    "    \n",
    "    target_ids_dict = dict()\n",
    "    unique_target_labels = set(targets)\n",
    "    _id = 0\n",
    "    \n",
    "    for ul in unique_target_labels:\n",
    "        target_ids_dict[ul] = _id\n",
    "        _id += 1\n",
    "        \n",
    "    # convert target labels to integers using the same dict\n",
    "    def convert_label_to_ids(targets, id_dict):\n",
    "        converted_targets = np.zeros(shape=(len(targets, )), dtype=np.int32)\n",
    "        for idx, target in tqdm(enumerate(targets)):\n",
    "            converted_targets[idx] = id_dict[target]\n",
    "\n",
    "        return converted_targets\n",
    "    \n",
    "    targets_converted = convert_label_to_ids(\n",
    "        targets=targets, id_dict=target_ids_dict)\n",
    "    \n",
    "    assert features.shape[0] == targets_converted.shape[0]\n",
    "    \n",
    "    # the features from the dataset are not normalised and\n",
    "    # this can cause probblems during training, such as \n",
    "    # gradients getting stuck in a local minima\n",
    "    # there's a lot of literature which talks about the \n",
    "    # necessity of normalisation, this is a good starter\n",
    "    # https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/\n",
    "    \n",
    "    features_norm = preprocessing.normalize(features, norm=\"l2\")\n",
    "    \n",
    "    \n",
    "    return (features_norm, targets_converted)\n",
    "    \n",
    "    \n",
    "X, y = load_data()\n",
    "print(X.shape) # type: ignore\n",
    "print(y.shape) # type: ignore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay splendid. Now to create the data split and also convert these numpy arrays to jax arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size : 233\n",
      "Test Size: 100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features_train, features_test, targets_train, targets_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "features_train = jnp.array(features_train)\n",
    "features_test = jnp.array(features_test)\n",
    "targets_train = jnp.array(targets_train)\n",
    "targets_test = jnp.array(targets_test)\n",
    "\n",
    "\n",
    "print(f\"Train Size : {features_train.shape[0]}\")\n",
    "print(f\"Test Size: {features_test.shape[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN\n",
    "\n",
    "This is going to be a 2 layer ANN (with bias) and ReLU as activation, 3 target classes and cross entropy as the loss function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old ritual of generating prngs\n",
    "key = jax.random.PRNGKey(42)\n",
    "key, *subkeys = jax.random.split(key, num=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANN definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w1': Array([[-0.08597513, -0.6448704 ,  0.23621447,  0.29353538],\n",
       "        [-0.92236894, -0.58683443, -0.11856903, -0.48287952],\n",
       "        [-0.9644534 ,  0.5944211 ,  0.01106676,  1.1230296 ],\n",
       "        [-0.3575936 , -0.91668224, -1.1935288 ,  0.37115386]],      dtype=float32),\n",
       " 'b1': Array([[-0.12579788,  0.25644642, -0.19384825, -0.7098966 ]], dtype=float32),\n",
       " 'w2': Array([[-1.8784473 ,  0.4934729 , -2.0973306 ],\n",
       "        [ 0.10150066,  0.62454844,  0.36210752],\n",
       "        [ 0.6336977 ,  0.9954951 ,  0.7572251 ],\n",
       "        [-0.56333786,  0.721915  ,  1.315561  ]], dtype=float32),\n",
       " 'b2': Array([[0.17419021, 0.21936692, 1.7175494 ]], dtype=float32)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inits params\n",
    "# w, b\n",
    "# init strategy : Kaiming\n",
    "def ann(in_features, hidden_features, out_features, *prngs):\n",
    "    scale_factor = jnp.sqrt(2/in_features)\n",
    "    \n",
    "    # layer 1\n",
    "    w1 = jax.random.normal(prngs[0], (in_features, hidden_features)) * scale_factor\n",
    "    b1 = jax.random.normal(prngs[1], (1, hidden_features))\n",
    "    \n",
    "    # layer 2\n",
    "    w2 = jax.random.normal(\n",
    "        prngs[2], (hidden_features, out_features)) * scale_factor\n",
    "    b2 = jax.random.normal(\n",
    "        prngs[3], (1, out_features))\n",
    "    \n",
    "    return {\n",
    "        \"w1\": w1, \n",
    "        \"b1\": b1, \n",
    "        \"w2\": w2,\n",
    "        \"b2\": b2\n",
    "    }\n",
    "    \n",
    "\n",
    "\n",
    "params = ann(4, 4, 3, *subkeys)\n",
    "params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "@jax.jit\n",
    "def forward(params, x): \n",
    "    # from layer 1\n",
    "    # xW + b\n",
    "    out1 = x @ params[\"w1\"] + params[\"b1\"]\n",
    "    out1 = jax.nn.relu(out1)\n",
    "    \n",
    "    # layer 2\n",
    "    out2 = out1 @ params[\"w2\"] + params[\"b2\"]\n",
    "    out2 = jax.nn.relu(out2)\n",
    "    \n",
    "    # apply softmax to convert to probability dist\n",
    "    # since the loss function is cross entropy\n",
    "    logits = jax.nn.softmax(out2)\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and grad\n",
    "\n",
    "The formal definition of cross entropy loss for a multiclass classification is this: \n",
    "$$\n",
    "ce = -\\sum_{c=1}^My_{t}\\log(p_{t})\n",
    "$$\n",
    "\n",
    "where, $t$ stands for the correct class\n",
    "\n",
    "$y_t$ is the correct label and $p_t$ is what a model predicted for $t$\n",
    "\n",
    "There is a nifty trick to it if you represent your classes with int ids as I have done above. The ids start from 0, so you can basically treat them as indexes. Using this, the cross entropy for an instance basically becomes\n",
    "\n",
    "$$\n",
    "ce = -ln(p_t)\n",
    "$$\n",
    "\n",
    "This trick works fine for single dimension multi class probabilities. I have never verified it outside course assignments or simple experiments. Then again, this notebook is just here to show you how jax works. In practice, it'll be buckwild to write everything from scratch. *Don't violate the DRY principle!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1.9060549, dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss function\n",
    "# for a single instance\n",
    "# will vmap for batches\n",
    "\n",
    "@jax.jit\n",
    "def cross_entropy(params, x, y):\n",
    "    logits = forward(params, x)[0] # these arrays ....\n",
    "    \n",
    "    return -jnp.log(logits[y])\n",
    "\n",
    "\n",
    "cross_entropy(params, features_train[0], targets_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1.3011823, dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jax.jit\n",
    "def calculate_loss(params, x_batched, y_batched):\n",
    "    batch_loss = jax.vmap(cross_entropy, in_axes=(None, 0, 0))(params, x_batched, y_batched)\n",
    "    return jnp.mean(batch_loss, axis=0)\n",
    "\n",
    "\n",
    "calculate_loss(params, features_train, targets_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "optim = optax.sgd(learning_rate=0.1)\n",
    "optim_state = optim.init(params)\n",
    "loss_grad_fn = jax.value_and_grad(calculate_loss)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-ing\n",
    "Finally! Der Zug ist da! (Oh wait..........no, it's late again.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batched\n",
    "@jax.jit\n",
    "def train_step(params, state, x_batched, y_batched):\n",
    "    loss_value, grad = loss_grad_fn(params, x_batched, y_batched)\n",
    "    update, state = optim.update(grad, state)\n",
    "    params = optax.apply_updates(params, update)\n",
    "    \n",
    "    return params, loss_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import trange\n",
    "\n",
    "\n",
    "def train(params, state, x_batched, y_batched, epochs, log_every_n_step):\n",
    "    losses = list() # to keep track of losses per epoch\n",
    "    steps = list()\n",
    "    \n",
    "    step_counter = 0\n",
    "    \n",
    "    for _ in trange(epochs):\n",
    "        params, loss_val = train_step(params, state, x_batched, y_batched)\n",
    "        \n",
    "        # log\n",
    "        if step_counter % log_every_n_step == 0:\n",
    "            losses.append(loss_val)\n",
    "            steps.append(step_counter)\n",
    "        step_counter += 1\n",
    "        \n",
    "    return params, losses, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dcb81cbe03d405b8ee2a94af2a881a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trained_params, losses, steps = train(params, optim_state, features_train, targets_train, 1000, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b1': Array([[-0.12579788,  0.25644642, -0.19384825, -0.7098966 ]], dtype=float32),\n",
       " 'b2': Array([[0.9915221 , 0.21321745, 0.90636414]], dtype=float32),\n",
       " 'w1': Array([[-0.08597513, -0.6448704 ,  0.23621447,  0.29353538],\n",
       "        [-0.92236894, -0.58683443, -0.11856903, -0.48287952],\n",
       "        [-0.9644534 ,  0.5944211 ,  0.01106676,  1.1230296 ],\n",
       "        [-0.3575936 , -0.91668224, -1.1935288 ,  0.37115386]],      dtype=float32),\n",
       " 'w2': Array([[-1.8784473 ,  0.4934729 , -2.0973306 ],\n",
       "        [ 0.10150066,  0.62454844,  0.36210752],\n",
       "        [ 0.6336977 ,  0.9954951 ,  0.7572251 ],\n",
       "        [-0.56333786,  0.721915  ,  1.315561  ]], dtype=float32)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]],\n",
       "\n",
       "       [[0.42060086, 0.19313312, 0.38626605]]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.vmap(forward, in_axes=(None, 0))(trained_params, features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
