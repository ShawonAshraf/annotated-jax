{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with ANN\n",
    "\n",
    "So far, you've seen how to define a model using jax, train the model, do backpropagation, and test it. Let's take it a bit further. In this notebook we'll be writing a simple ANN to classify penguin species using the [palmers penguin dataset](https://github.com/mcnakhaee/palmerpenguins).\n",
    "\n",
    "Let's load the data and process it to get started. (apparently the main focus of this notebook is to show you the ANN and not how you can load data, so, I'm going to short circuit the whole process a bit)\n",
    "\n",
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from palmerpenguins import load_penguins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
      "0  Adelie  Torgersen            39.1           18.7              181.0   \n",
      "1  Adelie  Torgersen            39.5           17.4              186.0   \n",
      "2  Adelie  Torgersen            40.3           18.0              195.0   \n",
      "4  Adelie  Torgersen            36.7           19.3              193.0   \n",
      "5  Adelie  Torgersen            39.3           20.6              190.0   \n",
      "\n",
      "   body_mass_g     sex  year  \n",
      "0       3750.0    male  2007  \n",
      "1       3800.0  female  2007  \n",
      "2       3250.0  female  2007  \n",
      "4       3450.0  female  2007  \n",
      "5       3650.0    male  2007  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94ac51819ba34f1eaec132d1501f6383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(333, 4)\n",
      "(333,)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    penguins = load_penguins() # penguins is a dataframe\n",
    "    penguins = penguins.dropna() # type: ignore\n",
    "    \n",
    "    # print the head of the dataframe to give some view\n",
    "    print(penguins.head()) # type: ignore\n",
    "    \n",
    "    # collect the feature columns\n",
    "    feature_columns = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
    "    # classification target\n",
    "    target_column = \"species\"\n",
    "    \n",
    "    # features and targets\n",
    "    features = penguins[feature_columns].values  # type: ignore\n",
    "    targets = penguins[target_column].values  # type: ignore\n",
    "    \n",
    "    # but here's a catch\n",
    "    # the targets are categorical, so we have two two options here\n",
    "    # one hot encode them, or, assign a numeric value to them and keep a dictionary\n",
    "    # with the target label to int mapping\n",
    "    # the second approach is easier xD\n",
    "    \n",
    "    target_ids_dict = dict()\n",
    "    unique_target_labels = set(targets)\n",
    "    _id = 0\n",
    "    \n",
    "    for ul in unique_target_labels:\n",
    "        target_ids_dict[ul] = _id\n",
    "        _id += 1\n",
    "        \n",
    "    # convert target labels to integers using the same dict\n",
    "    def convert_label_to_ids(targets, id_dict):\n",
    "        converted_targets = np.zeros(shape=(len(targets, )), dtype=np.int32)\n",
    "        for idx, target in tqdm(enumerate(targets)):\n",
    "            converted_targets[idx] = id_dict[target]\n",
    "\n",
    "        return converted_targets\n",
    "    \n",
    "    targets_converted = convert_label_to_ids(\n",
    "        targets=targets, id_dict=target_ids_dict)\n",
    "    \n",
    "    assert features.shape[0] == targets_converted.shape[0]\n",
    "    \n",
    "    # the features from the dataset are not normalised and\n",
    "    # this can cause probblems during training, such as \n",
    "    # gradients getting stuck in a local minima\n",
    "    # there's a lot of literature which talks about the \n",
    "    # necessity of normalisation, this is a good starter\n",
    "    # https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/\n",
    "    \n",
    "    features_norm = preprocessing.normalize(features, norm=\"l2\")\n",
    "    \n",
    "    \n",
    "    return (features_norm, targets_converted)\n",
    "    \n",
    "    \n",
    "X, y = load_data()\n",
    "print(X.shape) # type: ignore\n",
    "print(y.shape) # type: ignore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay splendid. Now to create the data split and also convert these numpy arrays to jax arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size : 233\n",
      "Test Size: 100\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features_train, features_test, targets_train, targets_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "features_train = jnp.array(features_train)\n",
    "features_test = jnp.array(features_test)\n",
    "targets_train = jnp.array(targets_train)\n",
    "targets_test = jnp.array(targets_test)\n",
    "\n",
    "\n",
    "print(f\"Train Size : {features_train.shape[0]}\")\n",
    "print(f\"Test Size: {features_test.shape[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN\n",
    "\n",
    "This is going to be a 2 layer ANN (with bias) and ReLU as activation, 3 target classes and cross entropy as the loss function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PRNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old ritual of generating prngs\n",
    "key = jax.random.PRNGKey(42)\n",
    "key, *subkeys = jax.random.split(key, num=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANN definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w1': Array([[-0.08597513, -0.6448704 ,  0.23621447,  0.29353538],\n",
       "        [-0.92236894, -0.58683443, -0.11856903, -0.48287952],\n",
       "        [-0.9644534 ,  0.5944211 ,  0.01106676,  1.1230296 ],\n",
       "        [-0.3575936 , -0.91668224, -1.1935288 ,  0.37115386]],      dtype=float32),\n",
       " 'b1': Array([[-0.12579788,  0.25644642, -0.19384825, -0.7098966 ]], dtype=float32),\n",
       " 'w2': Array([[-1.8784473 ,  0.4934729 , -2.0973306 ],\n",
       "        [ 0.10150066,  0.62454844,  0.36210752],\n",
       "        [ 0.6336977 ,  0.9954951 ,  0.7572251 ],\n",
       "        [-0.56333786,  0.721915  ,  1.315561  ]], dtype=float32),\n",
       " 'b2': Array([[0.17419021, 0.21936692, 1.7175494 ]], dtype=float32)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inits params for a layer\n",
    "# returns the params as an array\n",
    "# w, b\n",
    "# init strategy : Kaiming\n",
    "def ann(in_features, hidden_features, out_features, *prngs):\n",
    "    scale_factor = jnp.sqrt(2/in_features)\n",
    "    \n",
    "    # layer 1\n",
    "    w1 = jax.random.normal(prngs[0], (in_features, hidden_features)) * scale_factor\n",
    "    b1 = jax.random.normal(prngs[1], (1, hidden_features))\n",
    "    \n",
    "    # layer 2\n",
    "    w2 = jax.random.normal(\n",
    "        prngs[2], (hidden_features, out_features)) * scale_factor\n",
    "    b2 = jax.random.normal(\n",
    "        prngs[3], (1, out_features))\n",
    "    \n",
    "    return {\n",
    "        \"w1\": w1, \n",
    "        \"b1\": b1, \n",
    "        \"w2\": w2,\n",
    "        \"b2\": b2\n",
    "    }\n",
    "    \n",
    "\n",
    "\n",
    "params = ann(4, 4, 3, *subkeys)\n",
    "params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(233, 1, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass\n",
    "@jax.jit\n",
    "def forward(params, x): \n",
    "    # from layer 1\n",
    "    # xW + b\n",
    "    out1 = x @ params[\"w1\"] + params[\"b1\"]\n",
    "    out1 = jax.nn.relu(out1)\n",
    "    \n",
    "    # layer 2\n",
    "    out2 = out1 @ params[\"w2\"] + params[\"b2\"]\n",
    "    out2 = jax.nn.relu(out2)\n",
    "    \n",
    "    # apply softmax to convert to probability dist\n",
    "    # since the loss function is cross entropy\n",
    "    logits = jax.nn.softmax(out2)\n",
    "    \n",
    "    return logits\n",
    "\n",
    "jax.vmap(forward, in_axes=(None, 0))(params, features_train).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and grad\n",
    "\n",
    "The formal definition of cross entropy loss for a multiclass classification is this: \n",
    "$$\n",
    "ce = -\\sum_{c=1}^My_{t}\\log(p_{t})\n",
    "$$\n",
    "\n",
    "where, $t$ stands for the correct class\n",
    "\n",
    "$y_t$ is the correct label and $p_t$ is what a model predicted for $t$\n",
    "\n",
    "There is a nifty trick to it if you represent your classes with int ids as I have done above. The ids start from 0, so you can basically treat them as indexes. Using this, the cross entropy for an instance basically becomes\n",
    "\n",
    "$$\n",
    "ce = -ln(p_t)\n",
    "$$\n",
    "\n",
    "This trick works fine for single dimension multi class probabilities. I have never verified it outside course assignments or simple experiments. Then again, this notebook is just here to show you how jax works. In practice, it'll be buckwild to write everything from scratch. *Don't violate the DRY principle!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.3626955, dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss function\n",
    "# for a single instance\n",
    "# will vmap for batches\n",
    "\n",
    "@jax.jit\n",
    "def cross_entropy(params, x, y):\n",
    "    logits = forward(params, x)[0] # these arrays ....\n",
    "    \n",
    "    return -jnp.log(logits[y])\n",
    "\n",
    "\n",
    "cross_entropy(params, features_train[0], targets_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1.2394663, dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@jax.jit\n",
    "def calculate_loss(params, x_batched, y_batched):\n",
    "    batch_loss = jax.vmap(cross_entropy, in_axes=(None, 0, 0))(params, x_batched, y_batched)\n",
    "    return jnp.mean(batch_loss)\n",
    "\n",
    "\n",
    "calculate_loss(params, features_train, targets_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update params during training\n",
    "# SGD\n",
    "# fixed learning rate, this ain't kaggle\n",
    "@jax.jit\n",
    "def update(params, gradients, lr=1e-3):\n",
    "    return jax.tree_map(\n",
    "        lambda p, g: p - lr * g, params, gradients\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-ing\n",
    "Finally! Der Zug ist da! (Oh wait..........no, it's late again.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_grad_fn = jax.value_and_grad(calculate_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batched\n",
    "@jax.jit\n",
    "def train_step(params, x_batched, y_batched):\n",
    "    loss_value, grad = loss_grad_fn(params, x_batched, y_batched)\n",
    "    updated_params = update(params, grad)\n",
    "    \n",
    "    return updated_params, loss_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import trange\n",
    "\n",
    "\n",
    "def train(params, x_batched, y_batched, epochs, log_every_n_step):\n",
    "    losses = list() # to keep track of losses per epoch\n",
    "    steps = list()\n",
    "    \n",
    "    step_counter = 0\n",
    "    \n",
    "    for _ in trange(epochs):\n",
    "        loss_val, params = train_step(params, x_batched, y_batched)\n",
    "        \n",
    "        # log\n",
    "        if step_counter % log_every_n_step == 0:\n",
    "            losses.append(loss_val)\n",
    "            steps.append(step_counter)\n",
    "        step_counter += 1\n",
    "        \n",
    "    return params, losses, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6abbf28df94997ba785d0aae3a9bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Using a non-tuple sequence for multidimensional indexing is not allowed; use `arr[tuple(seq)]` instead of `arr[seq]`. See https://github.com/google/jax/issues/4564 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trained_params, losses, steps \u001b[39m=\u001b[39m train(params, features_train, targets_train, \u001b[39m1000\u001b[39;49m, \u001b[39m50\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, x_batched, y_batched, epochs, log_every_n_step)\u001b[0m\n\u001b[1;32m      8\u001b[0m step_counter \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m trange(epochs):\n\u001b[0;32m---> 11\u001b[0m     loss_val, params \u001b[39m=\u001b[39m train_step(params, x_batched, y_batched)\n\u001b[1;32m     13\u001b[0m     \u001b[39m# log\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[39mif\u001b[39;00m step_counter \u001b[39m%\u001b[39m log_every_n_step \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(params, x_batched, y_batched)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39m@jax\u001b[39m\u001b[39m.\u001b[39mjit\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_step\u001b[39m(params, x_batched, y_batched):\n\u001b[0;32m----> 4\u001b[0m     loss_value, grad \u001b[39m=\u001b[39m loss_grad_fn(params, x_batched, y_batched)\n\u001b[1;32m      5\u001b[0m     updated_params \u001b[39m=\u001b[39m update(params, grad)\n\u001b[1;32m      7\u001b[0m     \u001b[39mreturn\u001b[39;00m updated_params, loss_value\n",
      "    \u001b[0;31m[... skipping hidden 20 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m, in \u001b[0;36mcalculate_loss\u001b[0;34m(params, x_batched, y_batched)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m@jax\u001b[39m\u001b[39m.\u001b[39mjit\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalculate_loss\u001b[39m(params, x_batched, y_batched):\n\u001b[0;32m----> 3\u001b[0m     batch_loss \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39;49mvmap(cross_entropy, in_axes\u001b[39m=\u001b[39;49m(\u001b[39mNone\u001b[39;49;00m, \u001b[39m0\u001b[39;49m, \u001b[39m0\u001b[39;49m))(params, x_batched, y_batched)\n\u001b[1;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m jnp\u001b[39m.\u001b[39mmean(batch_loss)\n",
      "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(params, x, y)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m@jax\u001b[39m\u001b[39m.\u001b[39mjit\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcross_entropy\u001b[39m(params, x, y):\n\u001b[0;32m----> 7\u001b[0m     logits \u001b[39m=\u001b[39m forward(params, x)[\u001b[39m0\u001b[39m] \u001b[39m# these arrays ....\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39mjnp\u001b[39m.\u001b[39mlog(logits[y])\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(params, x)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39m@jax\u001b[39m\u001b[39m.\u001b[39mjit\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(params, x): \n\u001b[1;32m      4\u001b[0m     \u001b[39m# from layer 1\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[39m# xW + b\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     out1 \u001b[39m=\u001b[39m x \u001b[39m@\u001b[39m params[\u001b[39m\"\u001b[39;49m\u001b[39mw1\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m+\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mb1\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      7\u001b[0m     out1 \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mrelu(out1)\n\u001b[1;32m      9\u001b[0m     \u001b[39m# layer 2\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/jax_examples/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:3942\u001b[0m, in \u001b[0;36m_rewriting_take\u001b[0;34m(arr, idx, indices_are_sorted, unique_indices, mode, fill_value)\u001b[0m\n\u001b[1;32m   3936\u001b[0m     \u001b[39mif\u001b[39;00m (\u001b[39misinstance\u001b[39m(aval, core\u001b[39m.\u001b[39mDShapedArray) \u001b[39mand\u001b[39;00m aval\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m () \u001b[39mand\u001b[39;00m\n\u001b[1;32m   3937\u001b[0m         dtypes\u001b[39m.\u001b[39missubdtype(aval\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39minteger) \u001b[39mand\u001b[39;00m\n\u001b[1;32m   3938\u001b[0m         \u001b[39mnot\u001b[39;00m dtypes\u001b[39m.\u001b[39missubdtype(aval\u001b[39m.\u001b[39mdtype, dtypes\u001b[39m.\u001b[39mbool_) \u001b[39mand\u001b[39;00m\n\u001b[1;32m   3939\u001b[0m         \u001b[39misinstance\u001b[39m(arr\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39mint\u001b[39m)):\n\u001b[1;32m   3940\u001b[0m       \u001b[39mreturn\u001b[39;00m lax\u001b[39m.\u001b[39mdynamic_index_in_dim(arr, idx, keepdims\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m-> 3942\u001b[0m treedef, static_idx, dynamic_idx \u001b[39m=\u001b[39m _split_index_for_jit(idx, arr\u001b[39m.\u001b[39;49mshape)\n\u001b[1;32m   3943\u001b[0m \u001b[39mreturn\u001b[39;00m _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,\n\u001b[1;32m   3944\u001b[0m                unique_indices, mode, fill_value)\n",
      "File \u001b[0;32m~/miniconda3/envs/jax_examples/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:4017\u001b[0m, in \u001b[0;36m_split_index_for_jit\u001b[0;34m(idx, shape)\u001b[0m\n\u001b[1;32m   4012\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Splits indices into necessarily-static and dynamic parts.\u001b[39;00m\n\u001b[1;32m   4013\u001b[0m \n\u001b[1;32m   4014\u001b[0m \u001b[39mUsed to pass indices into `jit`-ted function.\u001b[39;00m\n\u001b[1;32m   4015\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4016\u001b[0m \u001b[39m# Convert list indices to tuples in cases (deprecated by NumPy.)\u001b[39;00m\n\u001b[0;32m-> 4017\u001b[0m idx \u001b[39m=\u001b[39m _eliminate_deprecated_list_indexing(idx)\n\u001b[1;32m   4019\u001b[0m \u001b[39m# Expand any (concrete) boolean indices. We can then use advanced integer\u001b[39;00m\n\u001b[1;32m   4020\u001b[0m \u001b[39m# indexing logic to handle them.\u001b[39;00m\n\u001b[1;32m   4021\u001b[0m idx \u001b[39m=\u001b[39m _expand_bool_indices(idx, shape)\n",
      "File \u001b[0;32m~/miniconda3/envs/jax_examples/lib/python3.10/site-packages/jax/_src/numpy/lax_numpy.py:4297\u001b[0m, in \u001b[0;36m_eliminate_deprecated_list_indexing\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m   4293\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4294\u001b[0m     msg \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mUsing a non-tuple sequence for multidimensional indexing is not allowed; \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4295\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39muse `arr[array(seq)]` instead of `arr[seq]`. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4296\u001b[0m            \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/google/jax/issues/4564 for more information.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 4297\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[1;32m   4298\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4299\u001b[0m   idx \u001b[39m=\u001b[39m (idx,)\n",
      "\u001b[0;31mTypeError\u001b[0m: Using a non-tuple sequence for multidimensional indexing is not allowed; use `arr[tuple(seq)]` instead of `arr[seq]`. See https://github.com/google/jax/issues/4564 for more information."
     ]
    }
   ],
   "source": [
    "trained_params, losses, steps = train(params, features_train, targets_train, 1000, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
