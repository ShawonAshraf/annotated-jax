{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to load mini batches?\n",
    "\n",
    "The examples that I've shown so far are just toy examples. Nobody in real life will train neural networks with the entire dataset in one go or iterate through each data instance in the dataset. You use mini batches. \n",
    "\n",
    "Jax and the ecosystem of libaries were primarily built with doing computation (or using fancy words, doing machine learning research) in mind. As a result, Jax has nothing similar to Tensorflow Datasets or Pytorch data utils for structuring and loading data. Jax instead encourages to use PyTorch and TF for data loading.\n",
    "\n",
    "As outlined in [one of the official guides](https://jax.readthedocs.io/en/latest/notebooks/Neural_Network_and_Data_Loading.html), you can just use a pytorch dataset, and then override the collate_fn in the pytorch dataloader to use it with jax arrays. This may sound easy but for complex data cases writing a new collate_fn can be daunting and prone to errors. You can also use TF datasets but, personally, I'm not a big fan of TF and setting it up has been akin to offering peace [to some god of the pantheon](https://youtu.be/FxoFiu-M0mg?si=XFn7ZlB-RWNhFZmM) these days. So sticking with Pytorch, there must be an easier way right?\n",
    "\n",
    "Well turns out, someone has written a package named [jax-dataloader](https://github.com/birkhoffg/jax-dataloader), which can take care of the dataloading functionality for you. All you need is to define a torch dataset. (If you're visiting the jax-dataloader repo, give a star, and also contribute, we need more work like this).\n",
    "\n",
    "So in this notebook, I'll define a torch dataset and show you how to use jax-dataloader with it. The dataset will come from Huggingface Datasets package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da1d213ac2cf4d15b9f66b8cf2bc1079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db50260ced04251a0c2b8c32c95eb57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d78b2e2c24404e81b46d964032be8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/6.83k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffdac21000f044789ad5608f70fcc911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ffc8332b8d4283b75d16e99d8dfdcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/9.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c70784eed2be4a08ac1adc044625685c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/28.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82661a62c8b4300924827b4da6efcfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce942cb9469649f7b62ad355645916f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/4.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119b54c76bda42378705bf25d9a45203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7fd439bfa92472ba8bd22808a9057d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/60000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1fe42bb2e4404fa951c1a32cae2075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'label'],\n",
       "    num_rows: 60000\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_name = \"mnist\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.PngImagePlugin.PngImageFile'>\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# let's inspect a bit\n",
    "\n",
    "sample = dataset[0]\n",
    "print(type(sample[\"image\"]))\n",
    "print(type(sample[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.array(sample[\"image\"]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the torch dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, split) -> None:\n",
    "        self.dataset_name = \"mnist\"\n",
    "        self.split = split\n",
    "        \n",
    "        self.data = load_dataset(self.dataset_name, split=self.split)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data_instance = self.data[index]\n",
    "        \n",
    "        image = data_instance[\"image\"]\n",
    "        label = data_instance[\"label\"]\n",
    "        \n",
    "        # the image is a pillow image so we'll have to convert it to\n",
    "        # a numpy array\n",
    "        image = np.array(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# create a training dataset\n",
    "train_set = MNISTDataset(\"train\")\n",
    "print(type(train_set[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the dataloader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax_dataloader as jdl\n",
    "\n",
    "train_loader = jdl.DataLoader(dataset=train_set, backend=\"pytorch\", batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are a few important things to notice here. The API of `jdl.Dataloader` is quite similar to that of Pytorch Dataloader. However, there is a backend parameter. This is to tell the `jdl.Dataloader` that the dataset will come in a pytorch format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8), array([7, 8, 8, 0, 8, 4, 3, 9, 4, 3, 0, 6, 2, 1, 9, 6, 6, 7, 6, 5, 3, 8,\n",
      "       1, 1, 0, 3, 5, 4, 8, 3, 1, 4, 2, 2, 1, 0, 0, 6, 7, 5, 9, 5, 7, 1,\n",
      "       5, 7, 0, 7, 0, 7, 5, 0, 0, 6, 3, 6, 4, 0, 8, 1, 6, 9, 5, 1])]\n"
     ]
    }
   ],
   "source": [
    "# iterate through the loader\n",
    "for batch in train_loader:\n",
    "    print(batch)\n",
    "    break # let's stop here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if I don't want to use torch dataset?\n",
    "\n",
    "Good question. For simpler dataset structures you may not need something as expressive as a Dataset class. You can instead use the ArrayDataset type from jax-dataloader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(\n",
    "    [np.array(d[\"image\"]) for d in dataset]\n",
    ")\n",
    "\n",
    "labels = np.array([d[\"label\"] for d in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8), array([9, 0, 4, 8, 4, 4, 5, 8, 9, 2, 7, 2, 5, 5, 9, 1, 2, 2, 2, 4, 9, 7,\n",
      "       6, 8, 0, 9, 1, 4, 1, 6, 0, 0, 3, 0, 3, 7, 2, 7, 4, 8, 3, 7, 7, 1,\n",
      "       6, 8, 3, 5, 3, 2, 5, 4, 6, 1, 7, 3, 6, 5, 5, 4, 3, 5, 1, 9]))\n"
     ]
    }
   ],
   "source": [
    "arr_dataset = jdl.ArrayDataset(images, labels)\n",
    "# dataloader\n",
    "arr_loader = jdl.DataLoader(dataset=arr_dataset, backend=\"jax\", batch_size=64, shuffle=True)\n",
    "# iterate\n",
    "for batch in arr_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what has changed here? The backend param. It's no longer a pytorch dataset so you'll mention jax as a backend. There's another thing you can do with the ArrayDataset. Say, your dataset was originally given as pytorch tensors. How can you use them here? Just change the backend param. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8), array([9, 4, 1, 0, 4, 3, 1, 1, 7, 0, 9, 8, 5, 9, 7, 2, 9, 1, 2, 9, 2, 4,\n",
      "       4, 1, 4, 1, 3, 4, 7, 3, 2, 0, 5, 8, 9, 8, 2, 3, 1, 0, 1, 8, 7, 5,\n",
      "       8, 2, 9, 0, 0, 9, 2, 8, 4, 6, 0, 9, 5, 4, 5, 3, 2, 5, 5, 4])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "images_tensor = torch.from_numpy(images)\n",
    "labels_tensor = torch.from_numpy(labels)\n",
    "\n",
    "ds = jdl.ArrayDataset(images_tensor, labels_tensor)\n",
    "loader = jdl.DataLoader(dataset=ds, backend=\"pytorch\", batch_size=64, shuffle=True)\n",
    "for batch in loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
